<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="welcome.html">Welcome</a></li><li class="chapter-item expanded "><a href="lesson1/introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="lesson2/metrics-and-evaluation.html"><strong aria-hidden="true">2.</strong> Metrics and Evaluation</a></li><li class="chapter-item expanded "><a href="lesson3/pipelining.html"><strong aria-hidden="true">3.</strong> Pipelining</a></li><li class="chapter-item expanded "><a href="lesson4/branches.html"><strong aria-hidden="true">4.</strong> Branches</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lesson4/prediction.html"><strong aria-hidden="true">4.1.</strong> Prediction</a></li><li class="chapter-item expanded "><a href="lesson4/branch-target-buffer.html"><strong aria-hidden="true">4.2.</strong> Branch Target Buffer</a></li><li class="chapter-item expanded "><a href="lesson4/direction-predictor.html"><strong aria-hidden="true">4.3.</strong> Direction Predictor</a></li><li class="chapter-item expanded "><a href="lesson4/2-bit-predictor.html"><strong aria-hidden="true">4.4.</strong> 2 Bit Predictor</a></li><li class="chapter-item expanded "><a href="lesson4/history-based-predictors.html"><strong aria-hidden="true">4.5.</strong> History Based Predictors</a></li><li class="chapter-item expanded "><a href="lesson4/pshare.html"><strong aria-hidden="true">4.6.</strong> PShare</a></li><li class="chapter-item expanded "><a href="lesson4/tournament-predictor.html"><strong aria-hidden="true">4.7.</strong> Tournament Predictor</a></li><li class="chapter-item expanded "><a href="lesson4/return-address-stack.html"><strong aria-hidden="true">4.8.</strong> Return Address Stack</a></li></ol></li><li class="chapter-item expanded "><a href="lesson5/predication.html"><strong aria-hidden="true">5.</strong> Predication</a></li><li class="chapter-item expanded "><a href="lesson6/ilp.html"><strong aria-hidden="true">6.</strong> Instruction Level Parallelism</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lesson6/false-deps.html"><strong aria-hidden="true">6.1.</strong> False Dependencies</a></li><li class="chapter-item expanded "><a href="lesson6/what-is-ilp.html"><strong aria-hidden="true">6.2.</strong> What is ILP?</a></li></ol></li><li class="chapter-item expanded "><a href="lesson7/instruction-scheduling.html"><strong aria-hidden="true">7.</strong> Instruction Scheduling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lesson7/issuing.html"><strong aria-hidden="true">7.1.</strong> Tomasulo's Algorithm - Issuing</a></li><li class="chapter-item expanded "><a href="lesson7/dispatching.html"><strong aria-hidden="true">7.2.</strong> Tomasulo's Algorithm - Dispatching</a></li><li class="chapter-item expanded "><a href="lesson7/writing.html"><strong aria-hidden="true">7.3.</strong> Tomasulo's Algorithm - Writing</a></li><li class="chapter-item expanded "><a href="lesson7/review.html"><strong aria-hidden="true">7.4.</strong> Tomasulo's Algorithm - Review</a></li><li class="chapter-item expanded "><a href="lesson7/load-and-store.html"><strong aria-hidden="true">7.5.</strong> Tomasulo's Algorithm - Load and Store</a></li><li class="chapter-item expanded "><a href="lesson7/timing.html"><strong aria-hidden="true">7.6.</strong> Tomasulo's Algorithm - Timing</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#welcome" id="welcome">Welcome</a></h1>
<p>This notebook contains my personal notes for CS6290: High Performance Computer
Architecture offered at the Georgia Institute of Technology. A summary of the
course follows:</p>
<p>This course covers modern computer architecture, including branch prediction,
out-of-order instruction execution, cache optimizations, multi-level caches,
memory and storage, cache coherence and consistency, and multi- and many-core
processors.</p>
<h2><a class="header" href="#course-links" id="course-links">Course links</a></h2>
<ul>
<li><a href="https://omscs.gatech.edu/cs-6290-high-performance-computer-architecture">https://omscs.gatech.edu/cs-6290-high-performance-computer-architecture</a></li>
<li><a href="https://ebookcentral-proquest-com.prx.library.gatech.edu/lib/gatech/detail.action?docID=787253">https://ebookcentral-proquest-com.prx.library.gatech.edu/lib/gatech/detail.action?docID=787253</a></li>
</ul>
<h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<h2><a class="header" href="#what-is-computer-architecture" id="what-is-computer-architecture">What is computer architecture?</a></h2>
<p>We can think of computer architecture similar to how architecture is considered
for building. Buildings are designed to be well-suited for a specific purpose.
The same can be said of computers, as we design different computers for
different purposes. Some examples are desktop computers, laptops, and
cellphones; they all have different purposes and require different computer
architectures.</p>
<p><img src="lesson1/./img/what-is-computer-architecture.png" alt="what-is-computer-architecture" /></p>
<h2><a class="header" href="#why-do-we-need-computer-architecture" id="why-do-we-need-computer-architecture">Why do we need computer architecture?</a></h2>
<ol>
<li>To improve performance based upon some specific measure. These measures
could include:</li>
</ol>
<ul>
<li>Speed</li>
<li>Battery life</li>
<li>Size</li>
<li>Weight</li>
<li>Energy efficiency, etc.</li>
</ul>
<ol start="2">
<li>To improve the abilities provided by a computer. These abilities could include:</li>
</ol>
<ul>
<li>3D graphics</li>
<li>Debugging support</li>
<li>Security, etc.</li>
</ul>
<p>The first need for computer architecture is about making computers cheaper,
faster, smaller, etc. while the second need is based around making new things
possible/providing new functionality. Computer architecture utilizes discoveries
in fabrication technology and circuit design to achieve the goals stated above.</p>
<p><img src="lesson1/./img/why-do-we-need-computer-architecture.png" alt="why-do-we-need-computer-architecture" /></p>
<h2><a class="header" href="#computer-architecture-and-technology-trends" id="computer-architecture-and-technology-trends">Computer architecture and technology trends</a></h2>
<p>Computer architecture is about building future computers. The progress in
computer manufacturing technology is fast. We shouldn't design a new computer
with current technology and parts. By the time we've designed our new computer,
it's obsolete and using old technology. We need to track technology trends in
order to predict what's available in the future, allowing us to better design a
new computer using these new technologies.</p>
<p><img src="lesson1/./img/computer-arch-tech-trends.png" alt="computer-arch-tech-trends" /></p>
<h2><a class="header" href="#moores-law" id="moores-law">Moore's law</a></h2>
<p>Moore's law provides a good starting point for predicting what future computers
will look like based upon what is currently available today. As computer
architects, we can use these predictions to guide our expectations of what
technologies will be available for use when designing future computers. Below is
a high-level summary of Moore's law:</p>
<p><img src="lesson1/./img/moores-law.png" alt="moores-law" /></p>
<h2><a class="header" href="#the-memory-wall" id="the-memory-wall">The memory wall</a></h2>
<p>Processor speed doubles almost every two years, while memory capacity also
experiences the same phenomenon. Memory latency, however, has not kept pace with
processor speed and memory capacity. This is what's called the memory wall and,
in order to mitigate these differences in speed when processors need to access
memory, we have been using caches to close that gap. Caches can be thought of as
a series of stairs for memory access speed, and cache misses are the base of the
staircase with the slowest memory access speed possible. A high-level
illustration of the memory wall trend is provided below:</p>
<p><img src="lesson1/./img/memory-wall.png" alt="memory-wall" /></p>
<h2><a class="header" href="#power-consumption" id="power-consumption">Power consumption</a></h2>
<p>There are two kinds of power that a processor consumes:</p>
<ul>
<li>Dynamic (active) power - consumed by activity in a circuit</li>
<li>Static power - consumed when powered on but idle</li>
</ul>
<h3><a class="header" href="#active-power" id="active-power">Active power</a></h3>
<p>Below is an illustrative representation of the equation for calculating active
power consumption by a processor. The representation also displays how we can
calculate the change in active power consumption when a different chip
configuration is used, and if voltage and chip frequency are changed. An
explanation of each variable in the active power equation follows:</p>
<ul>
<li>capacitance - roughly proportional to chip area / larger chips will have more
capacitance</li>
<li>voltage - quadratic relationship between voltage and power consumption</li>
<li>frequency - clock frequency (GHz) of a processor</li>
<li>alpha - activity factor (percentage of processor transistors active for any
given clock cycle)</li>
</ul>
<p><img src="lesson1/./img/active-power.png" alt="active-power" /></p>
<h3><a class="header" href="#static-power" id="static-power">Static power</a></h3>
<p>Static power is power consumed while the processor is idle. Some of this is due
to the voltage being too low to prevent transistors from leaking/wasting energy,
but it can also come from other sources as well. Below is a high-level
representation identifying the relationship between static and active power as
voltage increases or decreases in the circuit. This representation also
identifies that there is some optimal voltage settings for circuits to avoid
utilize too much power in bot the active and static states.</p>
<p><img src="lesson1/./img/static-power.png" alt="static-power" /></p>
<h2><a class="header" href="#fabrication-cost-and-yield" id="fabrication-cost-and-yield">Fabrication cost and yield</a></h2>
<p>The cost to manufacture and convert silicone wafers to useable computer chips
is pretty much static. We, primarily, have to account for chip yield when a
silicone wafer is divided into a number computer chips - some of these chips
can have defects while others work fine. Below is high-level representation of
the silicone wafer to computer chip manufacturing process, and how the yield
is derived based upon the number of defects in wafer versus the number of chips
extracted from a wafer.</p>
<p><img src="lesson1/./img/fab-yield.png" alt="fab-yield" /></p>
<p>Below is an example of how we can calculate the fabrication cost per chip
based upon chip size. Smaller chips cost less to manufacture, while larger chips
cost the most. Smaller chips, over time according to Moore's Law, will be able
to do more for less as we are able to fit smaller transistors onto the chips.
Larger chips will be able to remain the same size and cost, however, they will
be faster overall due to Moore's Law.</p>
<p><img src="lesson1/./img/fab-cost.png" alt="fab-cost" /></p>
<h2><a class="header" href="#references" id="references">References</a></h2>
<ol>
<li><a href="lesson1/./pdf/Lesson1Notes.pdf">Lesson 1 Notes</a></li>
</ol>
<h1><a class="header" href="#metrics-and-evaluation" id="metrics-and-evaluation">Metrics and Evaluation</a></h1>
<p>This lesson covers latency and throughput, two metrics used to measure computer
performance.</p>
<h2><a class="header" href="#performance" id="performance">Performance</a></h2>
<ul>
<li><strong>Latency</strong> - the total time it takes for an operation to complete, from start
to finish.</li>
<li><strong>Throughput</strong> - <em>not the inverse of latency</em>; because operations can take
place concurrently within a pipeline, this can be thought of as the number of
operations completed per unit measure of time.</li>
</ul>
<p>Below is a high-level representation of this concept.</p>
<p><img src="lesson2/./img/latency-throughput.png" alt="latency-throughput" /></p>
<h2><a class="header" href="#comparing-performance" id="comparing-performance">Comparing performance</a></h2>
<p>Knowing how to measure performance, we can now compare the performance of two
systems using our metrics. We want to be able to make a definitive statement
about the <strong>speedup</strong> of a system in comparison to another system, basically
stating that system <strong>x</strong> is faster than system <strong>y</strong>. We can compute this with
both <strong>latency</strong> and <strong>throughput</strong>, however, our equations are slightly
different. Below is an image showing us the equations for computing <strong>speedup</strong>
using our two metrics.</p>
<p><img src="lesson2/./img/comparing-performance.png" alt="comparing-performance" /></p>
<h2><a class="header" href="#speedup" id="speedup">Speedup</a></h2>
<p>A <strong>speedup</strong> value larger than 1 means we have improved performance. With
improved performance, we achieve higher throughput and shorter execution time. A
speedup less than 1 means that we have worse performance. When computing
speedup, we need to remember that performance is directly proportional to
throughput and performance has an inverse relationship with latency.</p>
<p><img src="lesson2/./img/speedup.png" alt="speedup" /></p>
<h2><a class="header" href="#measuring-performance" id="measuring-performance">Measuring performance</a></h2>
<p>What workload do we use to measure performance of different systems? We can't
use an actual user workload because:</p>
<ul>
<li>Many different users use computers in many different ways with many different
programs.</li>
<li>One workload will not be representative of all users.</li>
<li>How are we supposed to acquire the workload data?</li>
</ul>
<p>In order to solve this issue, we use <strong>benchmark</strong> workloads.</p>
<h2><a class="header" href="#benchmarks" id="benchmarks">Benchmarks</a></h2>
<p>Benchmarks are programs and input data that users and organizations have agreed
upon for use in performance measurements. Usually, we don't have just one
benchmark program but a <strong>benchmark suite</strong> consisting of multiple programs and
input data. Each program within a benchmark suite is representative of a type
of application.</p>
<h2><a class="header" href="#types-of-benchmarks" id="types-of-benchmarks">Types of benchmarks</a></h2>
<p>So what types of benchmarks are commonly used to measure performance? We have:</p>
<ul>
<li>
<p><strong>Real applications</strong></p>
<ul>
<li>Most representative of real workloads.</li>
<li>Also the most difficult to setup on new machines. Our testing environment
likely doesn't have an operating system, hardware, graphics processors, etc.</li>
</ul>
</li>
<li>
<p><strong>Kernels</strong></p>
<ul>
<li>The most time consuming portions of an application, usually a loop of some
sort. We've isolated these processing intensive sections of code to test our
machine. <em>Usually good for testing prototypes.</em></li>
</ul>
</li>
<li>
<p><strong>Synthetic benchmarks</strong></p>
<ul>
<li>Behave similar to kernels but are simpler to compile. We utilize these
benchmarks when testing early prototypes of a machine. <em>Usually good for design</em>
<em>studies.</em></li>
</ul>
</li>
<li>
<p><strong>Peak performance</strong></p>
<ul>
<li>Performance that's not based on running against actual code. The theoretical
highest number of instructions per second. <em>Usually good for marketing.</em></li>
</ul>
</li>
</ul>
<h2><a class="header" href="#benchmark-standard" id="benchmark-standard">Benchmark standard</a></h2>
<p>So how are benchmark suites created? Who makes them and what are the standards?
There exist standards organizations that receive input from manufacturers, user
groups, and experts in academia and these organizations produce standard
benchmark suites. Some well known standard benchmark suites are:</p>
<ul>
<li><strong>TPC</strong> - Benchmarks used for databases, web servers, data mining, and other
transaction processing. <a href="lesson2/metrics-and-evaluation.html#references">[1]</a></li>
<li><strong>EEMBC</strong> - Used for embedded processing. <a href="lesson2/metrics-and-evaluation.html#preferences">[2]</a></li>
<li><strong>SPEC</strong> - Used to evaluate engineering work stations and raw processors. SPEC
encompasses a large set of workloads, trying to cover a variety of uses for
processors in high performance systems. A breakdown of these workloads is in the
image below. <a href="lesson2/metrics-and-evaluation.html#preferences">[3]</a></li>
</ul>
<p><img src="lesson2/./img/benchmark-standard.png" alt="benchmark-standard" /></p>
<h2><a class="header" href="#summarizing-performance" id="summarizing-performance">Summarizing performance</a></h2>
<p>To summarize performance, we are looking for the <strong>average execution time</strong>.
A demonstration on how to calculate this is shown below. We should refrain from
averaging the speedups for each application tested - averaging ratios will not
provide useable data to summarize performance. In order to acquire the average
speedup, we need to use the <strong>geometric mean</strong> for the execution times of each
computer.</p>
<p><img src="lesson2/./img/summarizing-performance.png" alt="summarizing-performance" /></p>
<h2><a class="header" href="#iron-law-of-performance" id="iron-law-of-performance">Iron Law of performance</a></h2>
<p>Processor time (CPU time) can be expressed as:
<code>(instructions/program) * (cycles/instruction) * (seconds/cycles)</code></p>
<p>So why do we think about these components of processor time instead of just
measuring processor time directly? These three components allow us to think
about the different aspects of computer architecture.</p>
<ul>
<li><code>(instructions/program)</code> - influenced by the algorithm used to create the
program, the compiler used to interpret and generate the program, and the
instruction set being used.</li>
<li><code>(cycles/instruction)</code> - influenced by the instruction set being used, and
the processor design.</li>
<li><code>(seconds/cycle)</code> - influenced by the processor design, circuit design, and
transistor physics.</li>
</ul>
<p>Computer architecture primarily focuses on instruction set and processor design,
and good designs of these two aspects attempts to balance their effects on
CPU time.</p>
<p><img src="lesson2/./img/iron-law.png" alt="iron-law" /></p>
<h2><a class="header" href="#iron-law-for-unequal-instruction-times" id="iron-law-for-unequal-instruction-times">Iron Law for unequal instruction times</a></h2>
<p>It's pretty simple to calculate CPU times when we assume that all instructions
for a program will take the same amount of cycles to execute. This isn't always
the case, however. We need to be able to sum all the cycles for each type of
instruction before we multiply this with our time per cycle. Below is a
representation of this concept, showing us how to calculate a more realistic
CPU time.</p>
<p><img src="lesson2/./img/iron-law-unequal.png" alt="iron-law-unequal" /></p>
<p>Below is an example problem in which we calculate the sum of cycles for a
program in order to determine the CPU time.</p>
<p><img src="lesson2/./img/iron-law-quiz.png" alt="iron-law-quiz" /></p>
<h2><a class="header" href="#amdahls-law" id="amdahls-law">Amdahl's Law</a></h2>
<p>This law is useful when we need to calculate the overall speedup of the entire
program, but we only enhanced a fraction of the program. Below is an image
that attempts to explain the equation for Amdahl's Law, a description of each
variable in the equation is provided:</p>
<ul>
<li><code>(1 - frac_enh)</code> - the fraction of the program that wasn't enhanced.</li>
<li><code>frac_enh</code> - the fraction of the program that was enhanced.</li>
<li><code>speedup_enh</code> - the speedup achieved due to the enhancement.</li>
</ul>
<p>It's very important to understand that <code>frac_enh</code> is a percentage of the
original execution time that is affected by the enhancement.</p>
<p><img src="lesson2/./img/amdahls-law.png" alt="amdahls-law" /></p>
<h3><a class="header" href="#implications" id="implications">Implications</a></h3>
<p>It's important to aim for enhancements that achieve a speedup on a larger
percentage of execution time for a program. This is demonstrated mathematically
below.</p>
<p><img src="lesson2/./img/amdahls-law-implications.png" alt="amdahls-law-implications" /></p>
<p>Below is an example of how to use Amdahl's Law to compare multiple possible
improvements.</p>
<p><img src="lesson2/./img/amdahls-law-quiz.png" alt="amdahls-law-quiz" /></p>
<h2><a class="header" href="#lhadmas-law" id="lhadmas-law">Lhadma's Law</a></h2>
<p>This law is jokingly used to express the opposite of Amdahl's Law. While
Amdahl's Law says to optimize for the common case, Lhadma's warns us that we
should avoid attempting to optimize too much at the expense of other parts of
our performance. An example is provided below.</p>
<p><img src="lesson2/./img/lhadmas-law.png" alt="lhadmas-law" /></p>
<h2><a class="header" href="#diminishing-returns" id="diminishing-returns">Diminishing returns</a></h2>
<p>This concept covers the idea that as computer architects, we need to
continuously review what needs to be enhanced within a system instead of
continuously enhancing the same portion of a system. This concept stems from
Amdahl's Law, as it describes that, eventually, the enhanced portion of a
system will become smaller as we apply enhancements across generations. We will
achieve a diminished speedup if we continue to enhance the same portion of the
system. We need to continually reassess what is the common case when conducting
our enhancements. Below is a high level representation of this idea.</p>
<p><img src="lesson2/./img/diminishing-returns.png" alt="diminishing-returns" /></p>
<h2><a class="header" href="#references-1" id="references-1">References</a></h2>
<ol>
<li><a href="http://www.tpc.org/tpch/">http://www.tpc.org/tpch/</a></li>
<li><a href="https://www.eembc.org/">https://www.eembc.org/</a></li>
<li><a href="https://www.spec.org/benchmarks.html">https://www.spec.org/benchmarks.html</a></li>
<li><a href="lesson2/./pdf/Lesson2Notes.pdf">Lesson 2 Notes</a></li>
</ol>
<h1><a class="header" href="#pipelining" id="pipelining">Pipelining</a></h1>
<p>This lesson reviews pipelining to set the stage for more advanced topics.</p>
<h2><a class="header" href="#pipelining-in-a-processor" id="pipelining-in-a-processor">Pipelining in a processor</a></h2>
<p>This section covers basic pipelining in a processor. Most processors are much
more complex than the example provided here, however, this is used to review
content for students.</p>
<p>In a traditional processor pipeline, we have are series of stages. The following
listing of stages is not <em>all</em> stages, but it encompasses the important ones:</p>
<ul>
<li><strong>fetch</strong></li>
<li><strong>read</strong></li>
<li><strong>decode</strong></li>
<li><strong>execute</strong></li>
<li><strong>memory access</strong></li>
<li><strong>write</strong></li>
</ul>
<p>So how does pipelining apply to these stages? Instead of fetching, decoding, and
executing one instruction at a time, while one instruction is being decoded,
another instruction can be fetched from instruction memory. Then, when one
instruction is being executed, we can be decoding the instruction behind it.
So, while the latency may not change, the throughput of instructions through
the pipeline increases. Below is a high level representation of this concept:</p>
<p><img src="lesson3/./img/pipelining-in-processor.png" alt="pipeline-in-processor" /></p>
<p>Below is an example of calculating the latency of process with and without a
pipeline.</p>
<p><img src="lesson3/./img/laundry-pipelining.png" alt="laundry-pipelining" /></p>
<p>Below is a similar example as the one above, however, this one applies to
instructions and cycles.</p>
<p><img src="lesson3/./img/instruction-pipelining.png" alt="instruction-pipelining" /></p>
<h2><a class="header" href="#pipeline-cycles-per-instruction" id="pipeline-cycles-per-instruction">Pipeline cycles per instruction</a></h2>
<p>Throughout these notes we've been assuming one cycle per instruction, or a CPI
of 1, when our pipeline is full. In the real-world, however, we'll have billions
of instructions to execute - will our CPI always be 1? Here are some reasons
why our CPI might not be 1:</p>
<ul>
<li><strong>initial fill</strong> - when the pipeline initially fills up, our CPI will not be
equal to 1. Regardless, as our instruction number reaches infinity, CPI will
begin to approach 1.</li>
<li><strong>pipeline stalls</strong> - there exists the possibility that a fault occurs in the
pipeline and an instruction stalls, causing it to have to remain at that stage
for a cycle.</li>
</ul>
<p>Below is a high-level representation of how a CPI can be greater than 1.</p>
<p><img src="lesson3/./img/pipeline-cpi.png" alt="pipeline-cpi" /></p>
<h2><a class="header" href="#processor-pipeline-stalls" id="processor-pipeline-stalls">Processor pipeline stalls</a></h2>
<p>A processor pipeline stall usually occurs when some instructions depend upon the
outcome of previous instructions that conduct a read/write. In the example
below, the program loads a value into a register, increments the register, and
then stores that value into a different register. The load operation must occur
before the increment and load instruction, otherwise the increment instruction
will be incrementing an incorrect value. Because of this dependency, a processor
pipeline stall occurs, and a <strong>bubble</strong> in the pipeline is created. The
increment instruction must wait two cycles until the memory is read and written
into the register that is to be incremented.</p>
<p>This phenomenon causes our CPI to be greater than 1.</p>
<p><img src="lesson3/./img/processor-pipeline-stalls.png" alt="processor-pipeline-stalls" /></p>
<h2><a class="header" href="#processor-pipeline-stalls-and-flushes" id="processor-pipeline-stalls-and-flushes">Processor pipeline stalls and flushes</a></h2>
<p>A processor pipeline flush occurs when the processor pipeline fetches and
decodes instructions that aren't actually supposed to be executed, so they're
removed from the pipeline and replaced with bubbles. Below is an example
demonstrating what happens when a <code>JMP</code> instruction is introduced into the
pipeline. Some instructions behind the <code>JMP</code> are fetched and decoded, however,
they are fetched from an incorrect location in memory. After the <code>ALU</code>
determines the destination of the <code>JMP</code>, instructions from the <code>JMP</code> destination
are fetched and decoded, and the instructions that weren't destined to be
executed are flushed from the pipeline.</p>
<p>This is another phenomenon that could cause the CPI to be larger than 1.</p>
<p><img src="lesson3/./img/processor-pipeline-flushes.png" alt="processor-pipeline-flushes" /></p>
<h2><a class="header" href="#control-dependencies" id="control-dependencies">Control dependencies</a></h2>
<p>The problems described in the previous sections that cause these processor
pipeline stalls are called <strong>control dependencies</strong>. The example provided below
provides a high-level representation of a control dependency. In the scenario,
a branch instruction will jump to some <strong>label</strong> in code, however, to sections
of code depend upon the branch: the code directly after the branch and the code
contained at the label.</p>
<p>This example also shows us how to predict the CPI based upon this concept of
control dependencies. Given a percentage of instructions are branches, given
that a percentages of branches are actually taken, and given that we know that
the fetching and decoding of control dependencies causes at least two bubbles in
the pipeline, we can calculate the increase in CPI from its normal value of 1.</p>
<p>In later discussions, we will cover a concept called <strong>branch prediction</strong> that
is designed to mitigate the occurrence of these bubbles within the pipeline by
predicting where a branch will land in order to fetch and decode the correct
instructions. Control dependencies can cause even more bubbles to form in a
pipeline if the pipeline contains more than 5 stages, so these issues definitely
need to be mitigated to increase performance and normalize the CPI.</p>
<p><img src="lesson3/./img/control-dependencies.png" alt="control-dependencies" /></p>
<p>Below is an example question from the lectures that asks us to determine the CPI
given a percentage of branches/jumps taken and when the branches/jumps are
computed in the pipeline.</p>
<p><img src="lesson3/./img/control-dependencies-quiz.png" alt="control-dependencies-quiz" /></p>
<h2><a class="header" href="#data-dependencies" id="data-dependencies">Data dependencies</a></h2>
<p>Briefly described in <a href="lesson3/pipelining.html#processor-pipeline-stalls">this</a> section,
<strong>data dependencies</strong> occur when one instruction depends upon the outcome of
another instruction. TODO types of dependencies exist:</p>
<ul>
<li><strong>read after write (RAW)</strong> - type of dependency in which one instruction
relies upon the previous instruction writing some data that will be used by the
dependent instruction. This type of dependency is also called a <strong>flow</strong>
dependence because the data flows from one instruction to the other. This type
of dependency is also called a <strong>true</strong> dependency because the value being used
by the dependent instruction doesn't exist until the previous instruction
writes.</li>
<li><strong>write after write (WAW)</strong> - type of dependency in which the order of write
operations needs to be preserved. The example provided uses registers, in which
a specific value is expected to be within <code>R1</code> for future instructions. These
writes will not be able to to be conducted out of order - they both write to
the same location. This type of dependency is also called an <strong>output</strong>
dependency.</li>
<li><strong>write after read (WAR)</strong> - type of dependency in which a previous
instruction needs to read some data, but a future instruction intends to write
to that data. These instructions can not be interleaved, the previous
instruction expects the data to be unchanged upon read. The read will take place
before the write, creating the dependency. This type of dependency is also
called an <strong>anti-dependency</strong> because it reversed the <strong>RAW</strong> dependency.</li>
</ul>
<p>The <strong>WAW</strong> and <strong>WAR</strong> dependencies are called  <strong>false</strong> or <strong>named</strong>
dependencies. <strong>Read after read (RAR)</strong> is not a dependency.</p>
<p><img src="lesson3/./img/data-dependencies.png" alt="data-dependencies" /></p>
<p>Below is an example from the lectures inspecting a series of instructions to
determine what data dependencies exist.</p>
<p><img src="lesson3/./img/data-dependencies-quiz.png" alt="data-dependencies-quiz" /></p>
<h2><a class="header" href="#data-dependencies-and-hazards" id="data-dependencies-and-hazards">Data dependencies and hazards</a></h2>
<p>A <strong>hazard</strong> is when a dependence results in incorrect execution of an
instruction. In the example provided below, there are three instructions that
have dependencies but their dependencies will not result in incorrect values
being used for execution.</p>
<p>The <code>DIV</code> instruction, however, will be using a <strong>stale</strong> value for <code>R4</code> when it
executes and writes to <code>R10</code> because, when the <code>DIV</code> instruction reaches the
<strong>decode</strong> stage of the pipeline, the <code>SUB</code> instruction has not yet written its
value to <code>R4</code>.</p>
<p>Hazards can both be a property of the program as well as because of the
pipeline. Another example provided in the image below shows that, in this 5
stage pipeline, true dependencies do not create a hazard when 3 or more
instructions separate the dependent instructions. This is because, by the time
the first instruction in the dependent pair executes and writes, the second
dependent instruction is still being fetched.</p>
<p><img src="lesson3/./img/hazards.png" alt="hazards" /></p>
<p>Below is an example problem with a 3 stage pipeline, demonstrating how we can
inspect instructions to determine dependencies and hazards.</p>
<p><img src="lesson3/./img/hazards-quiz.png" alt="hazards-quiz" /></p>
<h2><a class="header" href="#handling-of-hazards" id="handling-of-hazards">Handling of hazards</a></h2>
<p>We need to introduce mechanisms to handle hazards we detect in order to protect
our CPI and performance. We don't care about all dependencies that are
introduced, only the ones we know will cause incorrect execution. These are our
possible mitigation techniques:</p>
<ul>
<li><strong>Flush dependent instructions</strong> - used for control dependencies. We don't
intend to execute instructions introduced into the pipeline by control
dependencies.</li>
<li><strong>Stall dependent instructions</strong> - used for data dependencies in order to
prevent instructions from reading invalid values.</li>
<li><strong>Fix values read by dependent instructions</strong> - also used for data
dependencies, this introduces the concept of <strong>forwarding</strong>, providing the a
dependent instruction with the value it needs to correctly decode and execute.</li>
</ul>
<p>Below is a high-level representation of these hazard handling mechanisms.</p>
<p><img src="lesson3/./img/handling-hazards.png" alt="handling-hazards" /></p>
<p>Below is an example problem with a 5 stage pipeline and a series of instructions
containing multiple dependencies and hazards that must be avoided. This example
demonstrates how we can use the pipeline to determine when it is appropriate to
flush, stall, or forward to handle hazards.</p>
<p><img src="lesson3/./img/handling-hazards-quiz.png" alt="handling-hazards-quiz" /></p>
<h2><a class="header" href="#how-many-stages" id="how-many-stages">How many stages?</a></h2>
<p>In the pipelines we've reviewed, the ideal CPI is 1. Later in the course,
pipelines with a CPI higher than 1 are expected because the pipeline is
attempting to execute more than one instruction per cycle. Regardless, each
pipeline setup has an ideal CPI that it attempts to achieve.</p>
<p>So what happens if we add more stages? Well, we get more hazards. If a branch
is resolved in the third cycle of a pipeline, we only have to flush the two
previous instructions that were fetched and decoded. If a branch is resolved
in, for example, the tenth cycle of a pipeline, now 9 instructions have to be
flushed from the pipeline - kinda wasteful. With more hazards, our CPI also
increases.</p>
<p>Inversely, with more stages in our pipeline there's less work being done per
stage, decreasing our cycle time - we can execute cycles faster. Remember the
Iron Law?</p>
<p><code>CPU time = #instructions * CPI * cycle_time</code></p>
<p>If our number of instructions stays the same, but our CPI increases and our
cycle time decreases, we achieve a balance even with this longer pipeline. We
carefully choose the number of stages in our pipeline to balance the
relationship between CPI and cycle time.</p>
<p>Modern processors achieve the most performance with a processor pipeline of
30 - 40 stages as this strikes a perfect balance between cycle time and CPI.
This is great, but with an increase in number or processor pipeline stages, we
also draw a lot of power because we execute a lot of cycles per second. Thus,
a reasonable number of stages for the processor pipeline of modern processors is
10 - 15 stages as this strikes the best balance between performance and power.</p>
<p>Below is a high-level representation of the concepts described above.</p>
<p><img src="lesson3/./img/how-many-stages.png" alt="how-many-stages" /></p>
<h2><a class="header" href="#references-2" id="references-2">References</a></h2>
<ol>
<li><a href="lesson3/./pdf/Lesson3Notes.pdf">Lesson 3 Notes</a></li>
</ol>
<h1><a class="header" href="#branches" id="branches">Branches</a></h1>
<p>In the previous <a href="lesson4/../lesson3/pipelining.html">pipelining</a> lesson, we saw the
effects that hazards have on performance. Branches and jumps are common
instructions that introduce control dependencies - our only solution can't just
be to flush the processor pipeline each time a branch or jump is decoded. This
lesson covers techniques used to effectively avoid hazards introduced by control
dependencies.</p>
<h2><a class="header" href="#branch-in-a-pipeline" id="branch-in-a-pipeline">Branch in a pipeline</a></h2>
<p>The example provided below uses a 5 stage processor pipeline to demonstrate the
costs of incorrect branch prediction. The example explains how a branch
instruction works: is registers <code>R1</code> and <code>R2</code> are equal, the immediate value
represented by <code>Label</code> will be added to the program counter, <code>PC</code>, and execution
will begin at that location in memory. Otherwise, the program counter will be
incremented regularly - <code>R1</code> and <code>R2</code> are not equal.</p>
<p>In this scenario, a branch instruction enters the pipeline at cycle 1. The
branch instruction isn't evaluated until cycle 3 by the ALU. Meanwhile, two
instructions are fetched and decoded - these are represented by a <code>??</code> because
the instructions can be fetched either from memory directly after the branch
instruction or the landing location in memory if the branch instruction is
taken.</p>
<p>If we predict the outcome of the branch instruction correctly, the instructions
that were fetched and decoded will execute immediately after the branch
instruction leaves the pipeline - the branch will have taken 1 cycle to
complete. If we predicted the outcome of the branch incorrectly and loaded the
wrong instructions into the pipeline, these instructions will have to be flushed
and the branch will have taken 3 cycles to complete.</p>
<p><img src="lesson4/./img/branch-in-a-pipeline.png" alt="branch-in-a-pipeline" /></p>
<h2><a class="header" href="#references-3" id="references-3">References</a></h2>
<ol>
<li><a href="lesson4/./pdf/Lesson4Notes.pdf">Lesson 4 Notes</a></li>
</ol>
<h1><a class="header" href="#prediction" id="prediction">Prediction</a></h1>
<h2><a class="header" href="#branch-prediction-requirements" id="branch-prediction-requirements">Branch prediction requirements</a></h2>
<p>What do we need in order to successfully predict whether a branch will be taken
or not? What do need in order to determine where the branch is going if it's
taken?</p>
<p>The requirements are as follows:</p>
<ul>
<li>Branch prediction needs to work using only the knowledge of where we fetch the
current instruction from.
<ul>
<li>We need branch prediction to guess the program counter of the next
instruction to fetch.</li>
</ul>
</li>
<li>With branch prediction we must correctly guess:
<ul>
<li>Is this a branch?</li>
<li>If it is a branch, is it taken?</li>
<li>If it is a taken branch, what is the target program counter?</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#branch-prediction-accuracy" id="branch-prediction-accuracy">Branch prediction accuracy</a></h2>
<p>The image below demonstrates how we can calculate the theoretical CPI of a
processor using branch prediction given its branch prediction accuracy,
penalty per missed prediction, and percentage of instructions that constitute
branches within a benchmark program. The equation is contains these components:</p>
<ul>
<li><code>base CPI</code> - in the example below, this is <code>1</code>.</li>
<li><code>predictor accuracy</code> - this is the number of mis-predicted branches per
instruction.</li>
<li><code>penalty incurred</code> - the penalty is dictated by the number of stages in a
pipeline and when the branch instruction is evaluated.</li>
</ul>
<p>A more accurate branch predictor increases our performance, decreasing the
number of penalties taken for mis-predicted branches, thus decreasing our CPI.
The amount of help a better predictor has changes depending upon how long the
processor pipeline is. Longer pipelines benefit more from better branch
prediction than shorter ones.</p>
<p><img src="lesson4/./img/branch-prediction-accuracy.png" alt="branch-prediction-accuracy" /></p>
<p>Below is branch prediction example problem. The key to this problem is the fact
that no branch prediction is being conducted so, until each instruction is
decoded, another instruction will not be fetched, thus creating a bubble within
the pipeline. So, non-branch instructions will take 2 cycles to execute and
branch instructions will take 3 cycles to execute. Instructions will not be
fetched after a branch instruction is decoded because the branch instruction
still has to be evaluated prior to fetching from the possible branch
destination.</p>
<p>In contrast, each instruction for the perfect branch predictor takes 1
instruction, no bubbles exist within the processor pipeline. This is because
the branch predictor knows which instruction will be fetched next before the
previous one is decoded - including the branches.</p>
<p><img src="lesson4/./img/branch-prediction-benefit-quiz.png" alt="branch-prediction-benefit-quiz" /></p>
<h2><a class="header" href="#performance-with-not-taken-prediction" id="performance-with-not-taken-prediction">Performance with not-taken prediction</a></h2>
<p>A simple implementation of branch prediction that has an increase in performance
over just not predicting branches at all is the prediction that all branches
are <strong>not taken</strong>. If no predictions are made, ever, and instructions are only
fetched after previous instructions are decoded, branches will always take 3
cycles in our example 5 stage pipeline and regular instructions will take 2
cycles.</p>
<p>If we always assume the branch is not taken, sometimes we'll be right and the
branch will only use 1 cycle, while sometimes we'll be wrong and the branch
will use 3 cycles - this is still better than making no predictions at all. In
this model, regular instructions only take 1 cycle, as well.</p>
<p><img src="lesson4/./img/not-taken-prediction.png" alt="not-taken-prediction" /></p>
<h2><a class="header" href="#predict-not-taken" id="predict-not-taken">Predict not-taken</a></h2>
<p>The <strong>predict not-taken</strong> predictor is the simplest predictor we can have, all
it does is increment the program counter. No extra hardware is required to
support the predict not-taken predictor - we should always have the ability to
increment the program counter.</p>
<p>A rule of thumb in computer architecture is that
<strong>20 percent of all instructions are branches</strong>. For branches, about
<strong>60 percent of all branches are taken</strong>. Thus, the predict not-taken predictor
is correct 80 percent of the time (because of non-branch instructions), plus
another 8 percent of the time (because of branch instructions). Overall, the
predict not-taken predictor is incorrect 12 percent of the time.</p>
<p><img src="lesson4/./img/predict-not-taken.png" alt="predict-not-taken" /></p>
<h2><a class="header" href="#why-do-we-need-better-prediction" id="why-do-we-need-better-prediction">Why do we need better prediction?</a></h2>
<p>The image below provides examples for different types of pipelines, showcasing
the differences in performance using a predict not-taken predictor and a
branch predictor that is correct 99 percent of the time. As you can see, as the
processor pipeline gets longer and more complex, conducting more instructions
per cycle, branch prediction becomes really important for performance. The
ability to accurately predict branches achieves a speedup of 4 for the case
of the 14 stage pipeline with 4 instructions per cycle.</p>
<p><img src="lesson4/./img/better-prediction.png" alt="better-prediction" /></p>
<h2><a class="header" href="#predictor-impact-quiz" id="predictor-impact-quiz">Predictor impact quiz</a></h2>
<p>Below is an example quiz that demonstrates how we can determine the impact
of a predictor on the CPI of a processor pipeline. Given some metrics about the
Pentium 4 processor, we are given a final CPI but not the theoretically ideal
CPI of the pipeline. Using the metrics given, we have to derive the ideal CPI
and then calculate the new CPI given a different mis-prediction percentage for
our predictor.</p>
<p><img src="lesson4/./img/predictor-impact-quiz.png" alt="predictor-impact-quiz" /></p>
<h2><a class="header" href="#why-we-need-better-prediction-part-2" id="why-we-need-better-prediction-part-2">Why we need better prediction, part 2</a></h2>
<p>The below image displays how wasteful flushing the processor pipeline is when
we fail to accurately predict branches. Depending upon how many stages are
within a pipeline, while also accounting for the fact that we could execute
multiple instructions per cycle, we see that would fetch and flush anywhere from
2, to 10, to 40 instructions - pretty wasteful.</p>
<p><img src="lesson4/./img/better-prediction-2.png" alt="better-prediction-2" /></p>
<h2><a class="header" href="#how-do-we-get-better-prediction" id="how-do-we-get-better-prediction">How do we get better prediction?</a></h2>
<p>In a <a href="lesson4/prediction.html#branch-prediction-requirements">previous section</a>, we discussed the
requirements that needed to be met in order to conduct branch prediction. In the
image below, the lecture discusses that we don't have the ability to determine
if the instruction is a branch, if the instruction is taken, or what the offset
of the branch is because we still have yet to fetch the instruction.</p>
<p>What we do know, however, is that, based upon the program counter, we have a
historical precedence for what happened at this program counter in the past. We
know how the branch at this program counter was behaving in the past - we can
use this information because it's pretty common that branches tend to behave the
same way each time they are encountered.</p>
<p>We don't know what the current branch instruction to be fetched is going to do,
but we can make a prediction based upon its history / the last time it was
executed.</p>
<p><img src="lesson4/./img/better-prediction-3.png" alt="better-prediction-3" /></p>
<h1><a class="header" href="#branch-target-buffer-btb" id="branch-target-buffer-btb">Branch Target Buffer (BTB)</a></h1>
<p>The simplest predictor that uses history is the <strong>branch target buffer</strong>. So
how does it work? The program counter (PC) is used to index into the BTB.
Stored at this index is the predicted next program counter. The next program
counter is fetched into the pipeline. When the branch is calculated, the real
next program counter is determined and compared against the predicted next
program counter that was fetched. If they're the same, the BTB predicted the
next program counter correctly based upon the historical data saved for the
program counter used as an index. If they're not the same, the BTB predicted
incorrectly and the next program counter is saved to the BTB using the index
program counter.</p>
<p><img src="lesson4/./img/branch-target-buffer.png" alt="branch-target-buffer" /></p>
<p>How big does the BTB need to be to implement the predictor? Well it serves the
purpose of predicting the next instruction based upon the current instruction
in the program counter. So in order to feasibly predict every next instruction,
we need to have an index for every instruction in the program - meaning the
BTB needs to be the same size as the program itself! That's not realistic, let's
see if we can't find another answer.</p>
<p><img src="lesson4/./img/branch-target-buffer-size.png" alt="branch-target-buffer-size" /></p>
<h2><a class="header" href="#realistic-btb" id="realistic-btb">Realistic BTB</a></h2>
<p>First, we don't need an entry in the BTB for every single instruction in the
program - we can have the BTB contain only the instructions that are most
likely to be executed next. The BTB will operate somewhat like a cache, using
values that are most recently used. An example provided in the lecture is a
loop that contains, say, 100 instructions. As the loop executes, the BTB fills
with the loop's instructions and the BTB doesn't change for the duration of the
loop.</p>
<p>So how do we avoid program counter index collisions in the BTB? We use a
mapping function that's simple enough to execute within one cycle. In the
example provided, if the BTB contains 1024 entries, we use the last 10 bits of
our 64 bit program counters as indices. Why are we using the LSB of our program
counters? Because as a program increments, these values are what changes when
each instruction is executed.</p>
<p><img src="lesson4/./img/realistic-btb.png" alt="realistic-btb" /></p>
<h2><a class="header" href="#btb-quiz" id="btb-quiz">BTB Quiz</a></h2>
<p>In the quiz below, we would think that we would just use the last 10 bits of the
program counter, however, because our architecture enforces 4 bytes instructions
that are word-aligned, each entry in the BTB needs to be even and also divisible
by 4. So, in a BTB of 1024 entries, we would only be using 256 of the values -
that's a lot of wasted entries.</p>
<p>Our solution is to ignore the last 2 bits of the program counter and use the 10
bits to the left of these ignored bits - these bits are the ones that are most
likely to change. Using this indexing function, a program counter of
<code>0x0000ab0c</code> will be contained at BTB index <code>0x2c3</code>.</p>
<p><img src="lesson4/./img/btb-quiz.png" alt="btb-quiz" /></p>
<h1><a class="header" href="#direction-predictor" id="direction-predictor">Direction Predictor</a></h1>
<p>We further simplify our branch predictor by adding another abstraction, the
<strong>branch history table</strong>. This table contains a <code>1</code> or <code>0</code> for each different
program counter that indexes into it - we calculate indices just like we did
with the BTB (using the LSBits of a program counter that are most likely to be
unique).</p>
<p>The <code>0</code> represents that the branch is not taken. The program counter is
incremented and we begin fetching instructions from that location. No changes
are made to the BTB because no branch was taken.</p>
<p>The <code>1</code> represents that the branch is taken. The BTB is referenced to determine
the next program counter based upon the current one being inspected, and we
begin to fetch instructions from that new location.</p>
<p>If the prediction made is incorrect, the branch history table is updated with
the correct value at the index - <code>0</code> or <code>1</code>. If a not taken prediction was made
and this prediction ended up being incorrect, we update the BTB with the next
program counter that is the destination of the branch.</p>
<p>Because the branch history table is only 1 bit for each index, we can have way
more entries for unique program counters. This way, we can use the BTB only for
branches and not just regular instructions.</p>
<p><img src="lesson4/./img/direction-predictor.png" alt="direction-predictor" /></p>
<h2><a class="header" href="#btb--bht-quiz" id="btb--bht-quiz">BTB &amp; BHT Quiz</a></h2>
<p>The below quiz excerpt demonstrates that the BHT needs to be accessed for each
instruction.</p>
<p><img src="lesson4/./img/btb-bht-quiz.png" alt="btb-bht-quiz" /></p>
<h2><a class="header" href="#btb--bht-quiz-2" id="btb--bht-quiz-2">BTB &amp; BHT Quiz 2</a></h2>
<p>The below quiz excerpt demonstrates how the BHT is indexed using the program
counter.</p>
<p><img src="lesson4/./img/btb-bht-quiz-2.png" alt="btb-bht-quiz-2" /></p>
<h2><a class="header" href="#btb--bht-quiz-3" id="btb--bht-quiz-3">BTB &amp; BHT Quiz 3</a></h2>
<p>The below quiz excerpt demonstrates the conditions required to access the BTB.</p>
<p><img src="lesson4/./img/btb-bht-quiz-3.png" alt="btb-bht-quiz-3" /></p>
<h2><a class="header" href="#btb--bht-quiz-4" id="btb--bht-quiz-4">BTB &amp; BHT Quiz 4</a></h2>
<p>The below quiz excerpt demonstrates how the BTB is indexed using the program
counter.</p>
<p><img src="lesson4/./img/btb-bht-quiz-4.png" alt="btb-bht-quiz-4" /></p>
<h2><a class="header" href="#btb--bht-quiz-5" id="btb--bht-quiz-5">BTB &amp; BHT Quiz 5</a></h2>
<p>The below quiz excerpt demonstrates how we can calculate the number of
mis-predictions for each instruction using an imperfect BHT.</p>
<p><img src="lesson4/./img/btb-bht-quiz-5.png" alt="btb-bht-quiz-5" /></p>
<h2><a class="header" href="#problems-with-1-bit-prediction" id="problems-with-1-bit-prediction">Problems with 1 bit prediction</a></h2>
<p>So what's wrong with the 1 bit predictor? Why does its performance not satisfy
the needs of computer architecture designers, creating the need to research
pattern predictors?</p>
<p>The 1 bit predictor predicts these things well:</p>
<ul>
<li><strong>Branches that are always taken</strong></li>
<li><strong>Branches that are always not taken</strong></li>
<li><strong>Branches that are taken A LOT more often than not taken</strong></li>
<li><strong>Branches that are not taken A LOT more often than taken</strong></li>
</ul>
<p>The example below demonstrates how, when a 1 bit predictor encounters an
<strong>anomaly</strong> in predictions for a particular branch, this can incur two
mis-predictions.</p>
<p>The 1 bit predictor does not perform well when:</p>
<ul>
<li><strong>Branches are taken more often than not, but not by much</strong></li>
<li><strong>Branches are not taken more often than not, but not by much</strong></li>
<li><strong>Encountering short loops</strong></li>
</ul>
<p>The 1 bit predictor will be bad when:</p>
<ul>
<li><strong>Branches taken and not taken are about the same in number</strong></li>
</ul>
<p><img src="lesson4/./img/1-bit-predictor-problems.png" alt="1-bit-predictor-problems" /></p>
<h1><a class="header" href="#2-bit-predictor" id="2-bit-predictor">2 Bit Predictor</a></h1>
<p>A 2 bit predictor or 2 bit counter (2BP or 2BC) just adds another bit to the BHT
to implement a state machine. The first bit in the 2BP is the prediction bit,
and the second bit is called the <strong>hysteresis</strong> or conviction bit. These are the
states of the 2BP:</p>
<ul>
<li><code>00</code> - strong not-taken state</li>
<li><code>01</code> - weak not-taken state</li>
<li><code>10</code> - weak taken state</li>
<li><code>11</code> - strong taken state</li>
</ul>
<p>You can probably see where this is going. We start out in the strong not-taken
state for a program counter. When we mis-predict that a branch is not-taken, we
increment the hysteresis bit and transition to the weak not-taken state. We
still don't take the branch until we are proven incorrectly again. A
mis-prediction at this point will transition us to the weak taken state.</p>
<p>So unlike the 1 bit predictor, our affinity to take or not take a branch doesn't
flip as easily. This solves the problem we had earlier where, if we encountered
an anomaly for a branch that is usually taken or not taken, we won't incur a
penalty of two mis-predictions.</p>
<p><img src="lesson4/./img/2-bit-predictor.png" alt="2-bit-predictor" /></p>
<h2><a class="header" href="#2-bit-predictor-initialization" id="2-bit-predictor-initialization">2 Bit Predictor initialization</a></h2>
<p>The below excerpt from the lectures demonstrates that it's a good idea to
initialize the 2 bit predictor in a weak state, doesn't matter if it's taken or
not taken. This is because, at program start, we don't know the history of the
branches, yet, so starting in the weak state only incurs 1 mis-prediction
penalty while starting in a strong state incurs 2.</p>
<p>On the flip-side, if a branch flip-flops and we start in the weak state, we will
incur a mis-prediction penalty every single time we encounter the branch. This
isn't common behavior (fortunately).</p>
<p>The evidence provided above would give you the idea that we should always init
the 2 bit predictor in the weak state, but for the sake of practicality, it
really doesn't matter and most 2 bit predictors are initialized at <code>00</code>.</p>
<p><img src="lesson4/./img/2-bit-predictor-init.png" alt="2-bit-predictor-init" /></p>
<h2><a class="header" href="#2-bit-predictor-quiz" id="2-bit-predictor-quiz">2 Bit Predictor Quiz</a></h2>
<p>The below quiz excerpt from the lectures demonstrates how there is a worst-case
scenario for the 2 bit predictor, showcasing a series of branches that will
cause the predictor to mis-predict every branch.</p>
<p>The moral of the story is, <strong>every</strong> predictor will have a worst-case scenario.
A good predictor will make it difficult to find or generate this worst-case
sequence of branches.</p>
<p><img src="lesson4/./img/2-bit-predictor-quiz.png" alt="2-bit-predictor-quiz" /></p>
<h2><a class="header" href="#1-bits-2-bits-what-else" id="1-bits-2-bits-what-else">1 bits, 2 bits... what else?</a></h2>
<p>So we've seen that the 2 bit predictor is better than the 1 bit predictor at
handling anomalous outcomes. Does this get better if we add 3 bits to the
predictor? Well, maybe. Another bit will just increase the number of states,
making it more difficult to transition to another prediction - this is useful
if the anomalous behavior comes in streaks. This behavior is not common, however
, in programs. Adding another bit imposes more cost, as it requires more space
to store the information per program counter.</p>
<p>A 2 bit predictor meets the needs we identified earlier of handling anomalous
behavior, but what about that one case where the branch outcome kept
flip-flopping? Since increasing the number of bits to solve this problem doesn't
really do anything, what's the next solution?</p>
<p><img src="lesson4/./img/2-bit-predictor-summary.png" alt="2-bit-predictor-summary" /></p>
<h1><a class="header" href="#history-based-predictors" id="history-based-predictors">History Based Predictors</a></h1>
<p>History Based Predictors are designed to recognize and learn patterns based
upon the history of taken or not taken outcomes of a branch. Below is an example
that provides a high-level representation of the concept. The flip-floppy
branch outcomes are definitely predictable, just not with simple N-bit counters.</p>
<p><img src="lesson4/./img/history-based-predictors.png" alt="history-based-predictors" /></p>
<h2><a class="header" href="#1-bit-history-w-2bc" id="1-bit-history-w-2bc">1 Bit History w/ 2BC</a></h2>
<p>Below is an excerpt from the lectures that provides a high-level representation
of a BHT with 1 bit for history, and two 2BC's to manage the state for taken
and not taken branch outcomes.</p>
<p>What the 1 bit history 2BC BHT provides us is a predictor that is able to learn
the pattern of a series of branch outcomes. The 1 bit history bit is used to
index into the two 2BC's and, based upon the branch outcome, updates the state
of each 2BC.</p>
<p><img src="lesson4/./img/1-bit-history-2bc.png" alt="1-bit-history-2bc" /></p>
<h2><a class="header" href="#1-bit-history-quiz" id="1-bit-history-quiz">1 Bit History Quiz</a></h2>
<p>Below is an a quiz excerpt from the lectures demonstrating the calculation of
the number of mis-predictions for a pattern of branch outcomes following the
this pattern: <code>(NNT)*</code>. As we can see from the outcome of the calculations in
the quiz, the 1 Bit History with two 2BC's is not a good solution for this
pattern.</p>
<p><img src="lesson4/./img/1-bit-history-2bc-quiz.png" alt="1-bit-history-2bc-quiz" /></p>
<h2><a class="header" href="#2-bit-history-predictor" id="2-bit-history-predictor">2 Bit History Predictor</a></h2>
<p>The 2 Bit History Predictor works very similar to the 1 Bit History Predictor,
now we've just doubled the number of 2BCs. The 2 Bit History bits are used to
index into the four 2BCs. The total cost of the predictor is 10 bits per branch.
Below is a high-level representation of how a 2 Bit History Predictor can
perfectly predict the <code>(NNT)*</code> sequence.</p>
<p><img src="lesson4/./img/2-bit-history-predictor.png" alt="2-bit-history-predictor" /></p>
<p>Thinking more about our history predictors, some things become evident. For an
N-bit History Predictor:</p>
<ul>
<li>We can perfectly predict all patterns of length less than or equal to <code>N+1</code>.</li>
<li>The cost for an N-bit History Predictor is <code>N + 2 * 2^N</code> <strong>per entry</strong></li>
<li>This is wasteful, most of the 2BCs will go unused.</li>
</ul>
<p><img src="lesson4/./img/2-bit-history-predictor-waste.png" alt="2-bit-history-predictor-waste" /></p>
<h2><a class="header" href="#n-bit-history-predictor-quiz" id="n-bit-history-predictor-quiz">N-bit History Predictor Quiz</a></h2>
<p>The below quiz is an excerpt from the lecture, outlining the cost and waste of
different size N-bit History Predictors.</p>
<p><img src="lesson4/./img/n-bit-history-predictor-quiz.png" alt="n-bit-history-predictor-quiz" /></p>
<h2><a class="header" href="#history-predictor-quiz" id="history-predictor-quiz">History Predictor Quiz</a></h2>
<p>The below quiz is an excerpt from the lecture, demonstrating how to calculate
the number of entries in a history predictor necessary to support a branch
outcome pattern generated by these two <code>for</code> loops. As you can see, a lot of
2BCs are wasted just to support an 8 bit history.</p>
<p><img src="lesson4/./img/history-predictor-quiz.png" alt="history-predictor-quiz" /></p>
<h2><a class="header" href="#history-w-shared-counters" id="history-w-shared-counters">History w/ shared counters</a></h2>
<p>As we discussed earlier, N-bit history predictors use <code>2^N</code> counter per entry,
even when we only use <code>N</code> counters to solve a pattern of <code>N</code> branch outcomes.
The idea proposed is to share 2BCs between program counter entries. We only use
<code>N</code> counters for <code>N</code> branch outcomes, so with enough counters there's a
possibility that we can share without having conflicts.</p>
<p><img src="lesson4/./img/history-shared-counters.png" alt="history-shared-counters" /></p>
<p>In the excerpt below, we see that designers use the <strong>pattern history table</strong>
abstraction to maintain history pattern history information in a different table
, separating it from the BHT. This way, 2BCs can be shared across multiple
program counters.</p>
<p>Program counters reference into the PHT in the same way they did with the BTH
and BTB, using the least significant bits that are unique. Some translation
logic is used to <code>XOR</code> the bits of the program counter and the pattern history
information from the PHT to reference the appropriate 2BC for the program
counter's pattern.</p>
<p>In this example, the size calculation for these two tables is explained, and the
overall cost is much less than the previous implementation of the N-bit history
predictor.</p>
<p><img src="lesson4/./img/pattern-history-table.png" alt="pattern-history-table" /></p>
<p>The below excerpt showcases how the BHT is used more efficiently with this
design. Patterns that are simple will only end up using 1 2BC, while other
patterns that have longer patterns will be able to utilize more counters
because less space is taken up by the patterns of other branches. The only
cost appended to this design is the creation of the pattern history table. Keep
in mind, with the indexing logic shown, it is still possible for branch entries
to collide in the BHT.</p>
<p><img src="lesson4/./img/shared-counter-cost.png" alt="shared-counter-cost" /></p>
<h1><a class="header" href="#pshare-and-gshare" id="pshare-and-gshare">PShare and GShare</a></h1>
<p>The tail end of the
<a href="lesson4/./lesson4/../history-based-predictors.html">history based predictor</a> lesson
describes the beginnings of what's known as the <strong>PShare</strong>  predictor. What
does PShare stand for? <strong>Private</strong> history for each branch and
<strong>Shared</strong> counters so that different histories and counters might share the
same counter. The PShare predictor is good for flip-floppy branch outcomes and
8-iteration loops.</p>
<p>In contrast, we have <strong>GShare</strong> with <strong>Global</strong> history and <strong>shared</strong> counters.
What does global history mean? It means we have single history that is used to
predict all branch outcomes. So every history, regardless of program counter,
is used to calculate the counter for prediction. The GShare predictor is good
for <strong>correlated branches</strong>.</p>
<p>What's a correlated branch? These are branches whose decisions rely upon what
other branches in the program are doing. An example is provided in the excerpt
below, showcasing a decision statement where one of two actions will take place
depending upon the value of a <code>shape</code> variable. Only one branch will evaluate to
try, thus they are correlated branches.</p>
<p><img src="lesson4/./img/pshare-gshare.png" alt="pshare-gshare" /></p>
<h2><a class="header" href="#pshare-vs-gshare-quiz" id="pshare-vs-gshare-quiz">PShare vs GShare Quiz</a></h2>
<p>Below is a quiz excerpt from the class explaining the differences between
PShare and GShare. Given some C code, we are provided what the C code would
resemble in assembly and then asked to determine how many bits of history
would be required to support the branches in the code using both PShare and
GShare.</p>
<p>The quiz demonstrates that it would be easy to predict the first and last
branches and that they don't require any history support. The first branch will
have the outcome of not taken 1000 times, and the last branch will have the
outcome of taken 1000 times. The second branch, however, will flip-flop between
taken and not taken, as each different integer will evaluate as even or odd.
Thus our PShare predictor will need <code>1</code> bit of history to support perfect
prediction of this branch's outcomes.</p>
<p>In contrast, GShare will require <code>3</code> bits of history to support the branches on
a global scale and in order to correctly predict the branch outcomes. Again,
the first and last branch outcomes are unlikely to change, however, the second
branch will flip-flop thus we need <code>3</code> bits of history to represent the
correlation between these branches. When we see that the first branch was not
taken, we'll also see the second branch either being taken or not taken, and
if the first branch wasn't taken we know that the third branch is taken.</p>
<p><img src="lesson4/./img/pshare-vs-gshare.png" alt="pshare-vs-gshare" /></p>
<h2><a class="header" href="#pshare-or-gshare" id="pshare-or-gshare">PShare or GShare</a></h2>
<p>Historically, designers would pick one or the other of these history based
predictors. Eventually, designers realized they would need to use both as GShare
works well with predicting correlated branches and PShare works better with
self similar branches.</p>
<h1><a class="header" href="#tournament-predictor" id="tournament-predictor">Tournament Predictor</a></h1>
<p>As we discussed in the <a href="lesson4/./lesson4/../pshare.html">PShare and GShare</a> section,
designers desired to use two predictors to handle different cases of branches -
correlated branches and branches that were self similar. But how do we determine
which predictor will be used for a program counter (instruction) if we don't
know what type of branch it is ahead of time?</p>
<p>The high-level representation below explains how the GShare and PShare
predictors are both indexed by the program counter and each predictor provides
a predicted branch outcome for that program counter value. At the same time, we
use the program counter to index into a <strong>meta-predictor</strong> table that has a
2BC at each index. We use the <strong>meta-predictor</strong> to keep track of which shared
predictor gives us the correct answer for the program counter the most often.</p>
<p><img src="lesson4/./img/tournament-predictor.png" alt="tournament-predictor" /></p>
<h1><a class="header" href="#return-address-stack-ras" id="return-address-stack-ras">Return Address Stack (RAS)</a></h1>
<p>So far we've seen branch prediction done for branch and jump instructions, but
what about function <code>ret</code> instructions? Functions are designed to be called
anywhere, so their <code>ret</code> address will not always be the same location in code.
This makes them tougher to predict using our current methods of branch
prediction - how do we prevent from fetching instructions from the wrong
location when returning from a function?</p>
<p><img src="lesson4/./img/return-address-stack.png" alt="return-address-stack" /></p>
<p>The <strong>return address stack</strong> is a separate predictor used specifically for
predicting returns. It operates similar to the process stack in that it pushes
and pops return address for function calls onto the stack. The primary
difference between a process stack and the RAS is that the RAS is a very small
hardware structure designed to make a prediction very quickly.</p>
<p>So what happens when the RAS is full and we continue to call into more
functions? We have two options:</p>
<ul>
<li>Don't push any more return pointers into the RAS</li>
<li>Wrap around and overwrite RAS entries</li>
</ul>
<p>So, is it better to wrap around and overwrite RAS entries? The results of a quiz
from the class provides the justification for why it's better to wrap around
and overwrite RAS entries. In the provided justification below, we demonstrate
with a high-level diagram that if we don't push new entries into the RAS when
it's full, all we're doing is saving ourselves from mis-predicting older
functions calls for functions that aren't getting called often.</p>
<p>In contrast, if we wrap around the entries in the RAS, we're more effectively
using the entires we have in the RAS to correctly predict immediate function
call returns that are more likely to happen more often.</p>
<p><img src="lesson4/./img/ras-quiz.png" alt="ras-quiz" /></p>
<h2><a class="header" href="#so-how-do-we-know-the-instruction-is-a-ret" id="so-how-do-we-know-the-instruction-is-a-ret">So how do we know the instruction is a <code>ret</code>?</a></h2>
<p>Right, we can't push and pop values to/from the RAS without knowing if the
instruction is actually a <code>ret</code>, and we only learn what the instruction is if
we fetch and decode. Two methods exist:</p>
<ul>
<li>We can use a predictor to determine if the program counter contains a <code>ret</code>.
Once the program counter is seen, we now know this program counter contains an
instruction that is a <code>ret</code> and we can then push the value onto the RAS.</li>
<li>We can use <strong>pre-decoding</strong>!
<ul>
<li>The processor contains a cache that stores instructions that have been
fetched from memory. The processor fetches instructions from the cache unless
the processor incurs a cache miss - then the process will access memory to
acquire the next instruction.</li>
<li>Pre-decoding is a process in which we access memory to acquire the next
instruction, and as we acquire the next instruction we decode a little bit of it
to determine if it's a <code>ret</code> instruction. If it is, we store this information 
with the instruction in the cache.</li>
</ul>
</li>
</ul>
<p><img src="lesson4/./img/pre-decoding.png" alt="pre-decoding" /></p>
<h1><a class="header" href="#predication" id="predication">Predication</a></h1>
<h2><a class="header" href="#summary" id="summary">Summary</a></h2>
<p>This lesson covers more topics in relation to control dependencies and the
hazards they pose. We covered a bunch of different branch predictors in
<a href="lesson5/./lesson4/branches.html">lesson 4</a> and we noticed that some branches are really
difficult to predict, even when we use really sophisticated branch predictors.</p>
<p>This lesson showcases how the compiler can help us to completely avoid hard to
predict branches.</p>
<h2><a class="header" href="#predication-explained" id="predication-explained">Predication explained</a></h2>
<p>Like we covered in the previous lesson, with branch prediction we attempt to
guess the landing location of the branch and we begin fetching instructions from
that instruction sequence into the pipeline. If we are correct in our guess,
we incur no penalty. An incorrect guess incurs a penalty and, depending upon
how long our pipeline is, this penalty can be <em>huge</em>.</p>
<p>Contrast this with <strong>predication</strong>. In predication, we fetch instructions from
the instruction sequences of both possible landing locations for a branch. With
this technique, even if we guess correctly or incorrectly, we still have to toss
out 50% of the instructions that have been fetched into the processor pipeline.
We don't incur a penalty anymore, however, because we realistically aren't
making a guess, we're just saying yes to both outcomes of a branch instruction.</p>
<p>So if we're always incurring a penalty with predication, in what context is it
better than branch prediction? Here is a breakdown of the examples showcased in
the lecture:</p>
<ul>
<li><strong>Loops</strong> - best application is a branch predictor. Loops are easier to
predict as the number of iterations increases. With predication, we would be
continuously fetching instructions from the instruction sequence after the loop,
wasting a lot of effort as those instructions will continuously flushed from the
pipeline - the branch that goes back into the loop will be taken almost always.</li>
<li><strong>Function calls/ret</strong> - best application is a branch predictor. The function
will always return, taking the branch instruction. No reason to fetch
instructions from the instruction sequence after a ret.</li>
<li><strong>Large decision statements (if-then-else)</strong> - best application is a branch
predictor, dependent upon the size of the instruction sequence that comprises
the decision statement and the length of the processor pipeline. If two
directions in a decision statement were both 100 instructions long and the
processor pipeline was 50 stages long, if we mis-predict we only incur a penalty
of 50 stages. If we predict correctly, we incur no penalty. In contrast, a
predicator will always incur the penalty of 100 because we load instructions
from both possible instruction sequences following the branch instruction.</li>
<li><strong>Small decision statements (if-then-else)</strong> - the best application is
predication. If two instruction sequence outcomes for a branch are both 5
instructions long, we will always incur a 5 instruction penalty with
predication. If we use branch prediction and the processor pipeline is 50 stages
long, if we make a mis-prediction, we'll have wasted 50 stages. Dependent upon
how accurate the branch predictor is, we might be able to match the performance
of predication in this use case, however, the more inaccurate the branch
predictor the less viable an option it will be for small decision statements.</li>
</ul>
<p><img src="lesson5/./img/predication.png" alt="predication" /></p>
<h2><a class="header" href="#conditional-move" id="conditional-move">Conditional move</a></h2>
<p>This section discusses the conditional move instructions available in the <code>MIPS</code>
and <code>x86</code> instruction sets. The example below covers <code>MOVZ</code> and <code>MOVN</code>:</p>
<ul>
<li><code>MOVZ</code> - takes two sources and a destination register. If the third operand
is equal to <code>0</code>, the second operand is loaded into the destination register.</li>
<li><code>MOVN</code> - takes two sources and a destination register. If the third operand
is not equal to <code>0</code>, the second operand is loaded into the destination register.</li>
</ul>
<p>Example <code>x86</code> <code>CMOV</code> instructions are shown below.</p>
<p><img src="lesson5/./img/conditional-move.png" alt="conditional-move" /></p>
<h2><a class="header" href="#movz-movn-quiz" id="movz-movn-quiz">MOVZ MOVN quiz</a></h2>
<p>Below is quiz a on how to convert code that originally contains branch
instruction into code that uses conditional moves to avoid making predictions.
This code models a short decision statement that loads some values into
different variables based upon some condition statement.</p>
<p><img src="lesson5/./img/movz-movn-quiz.png" alt="movz-movn-quiz" /></p>
<h2><a class="header" href="#movz-movn-performance" id="movz-movn-performance">MOVZ MOVN performance</a></h2>
<p>The excerpt from the lectures below showcases a comparison of the performance
between traditional branch prediction and the translation we did of the
instructions to enable predication. Give a branch predictor that's correct
80% of the time and incurs a 40 instruction penalty if we encounter a
mis-prediction, we average the number of instructions that can executed between
the two branch instruction sequences, <code>5 * 0.5 == 2.5</code>, and then we throw in our
inaccuracy and penalty: <code>2.5 + 0.2 * 40 == 10.5</code>.</p>
<p>So, on average, the branch predictor in this example incurs <code>10.5</code> instructions
worth of work to evaluate and execute this condition statement. In contrast, for
predication, all of the instructions are fetched and executed because we've
translated the branch into conditional move instructions. We incur <code>4</code>
instructions worth of work to evaluate and execute this condition statement.</p>
<p><img src="lesson5/./img/movz-movn-performance.png" alt="movz-movn-performance" /></p>
<h2><a class="header" href="#movx-summary" id="movx-summary">MOVx summary</a></h2>
<p>To summarize predication using <code>MOVx</code>:</p>
<ul>
<li>Needs compiler support to translate eligible condition statements using <code>MOVx</code>
instructions rather than generating branch instructions</li>
<li>Removes hard-to-predict branches, increasing performance</li>
<li>More registers are needed in order to support predication using <code>MOVx</code>
<ul>
<li>Results from both instruction sequences have to be calculated and stored</li>
</ul>
</li>
<li>More instructions are executed
<ul>
<li>No branch prediction is conducted, both instruction sequences are executed
and their results are stored in registers</li>
<li><code>MOVx</code> is used to select the results of the condition statement</li>
</ul>
</li>
</ul>
<p>So what portions of this summarized list are absolutely necessary to implement
predication? Well:</p>
<ul>
<li>Compiler support is definitely necessary</li>
<li>The whole purpose of this is to remove hard-to-predict branches</li>
<li><strong>We don't need more registers to store our results - we can conduct
comparison directly against values in memory</strong></li>
<li><strong>We don't need to use <code>MOVx</code> to select results</strong></li>
</ul>
<p>How do we remove the unnecessary portions of the summarization above? <strong>We make
all of our instructions conditional!</strong> With this, we can achieve <em>full
predication</em> - but it requires extensive support in the instruction set.</p>
<h2><a class="header" href="#hardware-support-for-full-predication" id="hardware-support-for-full-predication">Hardware support for full predication</a></h2>
<p>Usually, we have a separate opcode for conditional move instructions. For <strong>full
predication</strong>, we add condition bits to <em>every instruction</em>. Below is an excerpt
from the lectures showcasing the Itanium instruction set's use of <strong>qualifying
predicates</strong> in its instructions to support full predication. Qualifying
predicates specify what register will be used to conduct a comparison for a
conditional move.</p>
<p><img src="lesson5/./img/full-pred-hw-support.png" alt="full-pred-hw-support" /></p>
<h2><a class="header" href="#full-predication-example" id="full-predication-example">Full predication example</a></h2>
<p>The excerpt below now shows our previous condition statement convert to a set
of instructions that uses full predication. The first instruction sets the
qualifying predicates, <code>p1</code> and <code>p2</code>, to <code>0</code> or <code>1</code> based upon the value of <code>R1</code>
. If <code>R1</code> is <code>0</code>, <code>p1</code> is set and the instruction predicated by <code>p1</code> will
actually store its value into <code>R3</code> after execution. If <code>R1</code> is not <code>0</code>, <code>p2</code> is
set and the instruction predicated by <code>p2</code> will store its value into <code>R2</code> after
execution.</p>
<p>So, in the original code with branch instructions, we have the possibility of
executing 2 or 3 instructions based upon our prediction, but we still incur a
penalty. In our previous examples using <code>MOVx</code>, we were able to translate the
condition statement into a set of 4 instructions. Now, with hardware support for
full predication, we are able to translate this condition statement into 3
instructions.</p>
<p><img src="lesson5/./img/full-predication-ex.png" alt="full-predication-ex" /></p>
<h2><a class="header" href="#full-predication-quiz" id="full-predication-quiz">Full predication quiz</a></h2>
<p>Below is a the full predication quiz solution from the lecture, conducting a
performance comparison between the original branch version of the condition
statement code to the full predication translation.</p>
<p><img src="lesson5/./img/full-predication-quiz.png" alt="full-predication-quiz" /></p>
<h2><a class="header" href="#references-4" id="references-4">References</a></h2>
<ol>
<li><a href="lesson5/./pdf/Lesson5Notes.pdf">Lesson 5 Notes</a></li>
</ol>
<h1><a class="header" href="#instruction-level-parallelism-ilp" id="instruction-level-parallelism-ilp">Instruction Level Parallelism (ILP)</a></h1>
<h2><a class="header" href="#parallel-instruction-execution" id="parallel-instruction-execution">Parallel instruction execution</a></h2>
<p>The below excerpt from the lectures showcases the hazards that can manifest due
to data dependencies in parallel processor pipelines. As you can see here
multiple instructions are destined to be executed and, if all instructions are
able to be executed in parallel, it will take 5 cycles to execute a number of
instructions. As the number of instructions approaches infinity, our CPI
approaches <code>0</code>.</p>
<p>This isn't realistic - some instructions will depend upon earlier instructions
to write values to registers before the instruction is executed. If this isn't
done, we will encounter data hazards wherein the registers we use for our
instructions will contain stale values, ultimately causing the program to
execute incorrectly.</p>
<p><img src="lesson6/./img/same-cycle-ints.png" alt="same-cycle-ints" /></p>
<h2><a class="header" href="#the-execute-stage" id="the-execute-stage">The execute stage</a></h2>
<p>In previous sections of this notebook we discussed forwarding, a technique that
can be used to resolve data dependencies and remove hazards. Unfortunately, this
technique wont' work for us when we're executing instructions in parallel. The
instruction that contains the data dependency will not receive the outcome of
the forwarded instruction before it executes because they are executed in
parallel.</p>
<p>In order to remove the hazard related to this data dependency, we must stall
the instruction until the previous instruction executes. Then, we will forward
the result of the executed instruction to the dependent instruction. This will
increase our CPI.</p>
<p><img src="lesson6/./img/the-execute-stage.png" alt="the-execute-stage" /></p>
<h2><a class="header" href="#raw-dependencies" id="raw-dependencies">RAW dependencies</a></h2>
<p>The below excerpt from the lectures showcases how we can calculate the CPI
for a set of instructions based upon the number of data dependencies that exists
between them. As you can see, the more RAW dependencies that exist, the higher
our CPI.</p>
<p><img src="lesson6/./img/raw-deps.png" alt="raw-deps" /></p>
<h2><a class="header" href="#waw-dependencies" id="waw-dependencies">WAW dependencies</a></h2>
<p>The below excerpt from the lectures showcases how a WAW dependency can manifest
due to a RAW dependency in previous instructions. The WAW dependency poses a
hazard because the previous instruction is stalled due to a RAW dependency. If
this goes ignored, the dependent instruction in the WAW dependency will write to
a register before the previous write, executing instructions out of order. In
order to solve this, the dependent instruction must be stalled twice so that the
previous instruction can write.</p>
<p><img src="lesson6/./img/waw-deps.png" alt="waw-deps" /></p>
<h2><a class="header" href="#dependency-quiz" id="dependency-quiz">Dependency quiz</a></h2>
<p>Below is a quiz from the lectures in the class that showcases how multiple RAW
and WAW dependencies are handled in the processor pipeline.</p>
<p><img src="lesson6/./img/dep-quiz.png" alt="dep-quiz" /></p>
<h1><a class="header" href="#false-dependencies" id="false-dependencies">False Dependencies</a></h1>
<p>We covered this in previous portions of this notebook, but we'll go over the
dependency types once again. We have <strong>true</strong> dependencies and <strong>false</strong> or
<strong>name</strong> dependencies. A true dependency is the RAW (read after write)
dependency - pretty obvious because this is how the program is actually intended
to execute. Both the WAR (write after read) and WAW (write after write)
dependencies are false dependencies. In both of these dependencies, we are
re-using registers for instructions when we could change the registers being
used to eliminate the false dependencies completely.</p>
<h2><a class="header" href="#duplicating-register-values" id="duplicating-register-values">Duplicating register values</a></h2>
<p>This is a complicated technique in which we save multiple <strong>versions</strong> of a
value for a register that is being written to. In the excerpt below, two
different writes occur for the register: <code>R4</code>. This register is used in two
different read operations, but what value will be used if the register is
written twice?</p>
<p>To remove the hazard posed by this WAW dependency, we store both versions of
<code>R4</code> and then each read operation will utilize the appropriate version based
upon the chronological order of the writes. The third instruction in this
example will use the outcome of the write in instruction two for its read
operation, even though instruction four writes to <code>R4</code> before instruction two
does. A future instruction will use the most previous version of <code>R4</code> for its
read operation, even though the write by instruction two occurs after the write
by instruction four.</p>
<p>Essentially, we are keeping track of all possible versions of a register in a
parallel pipeline and different read operations that require the value of
register will use the value generated by the most recent write operation.</p>
<p><img src="lesson6/./img/duping-registers.png" alt="duping-registers" /></p>
<h2><a class="header" href="#register-renaming" id="register-renaming">Register renaming</a></h2>
<p>So duplicating and managing register values can get pretty complicated and time
consuming. Another technique processors can use to remove hazards created by
data dependencies is <strong>register renaming</strong>.</p>
<p>In the below excerpt from the lectures, we define some terms:</p>
<ul>
<li><strong>architectural registers</strong> - these are register names used by the programmer
and the compiler.</li>
<li><strong>physical registers</strong> - these are the physical registers available to the
processor to actually store real values.</li>
<li><strong>register allocation table</strong> - this is a table used by the processor to
translate architectural register names to physical registers. This table defines
which physical register contains the data for a specified architectural
register.</li>
</ul>
<p>The processor rewrites the program it's executing to use the physical registers.
Through register renaming, it will have more registers available to store values
and avoid hazards presented by data dependencies.</p>
<p><img src="lesson6/./img/register-renaming.png" alt="register-renaming" /></p>
<h2><a class="header" href="#rat-example" id="rat-example">RAT example</a></h2>
<p>The below excerpt from the class showcases how a RAT works. Each time a new
value is produced, a different physical register is used to store that value.
Values for registers that are being read are acquired from the RAT after it
translates the architectural name to the physical register. We call the WAW and
WAR dependencies name dependencies because the name is the whole problem. If we
rename the register that the value is being written to for each write, the
instructions won't overwrite the work of the other instructions if they're
executing in parallel.</p>
<p>In this example, we can see that each write increments the physical register
number that's being written to. This removes the hazard of a register being
overwritten by a later instruction before an earlier instruction can conduct
its write. Because the writes are to different physical registers, when an
instruction is decoded and needs to read a register, the value it's reading is
going to be from the instruction prior, not the most recent write to the
register.</p>
<p><img src="lesson6/./img/rat-example.png" alt="rat-example" /></p>
<h2><a class="header" href="#register-renaming-quiz" id="register-renaming-quiz">Register renaming quiz</a></h2>
<p>Below is a quiz for register renaming from the lectures. Pretty straightforward.</p>
<p><img src="lesson6/./img/register-renaming-quiz.png" alt="register-renaming-quiz" /></p>
<h2><a class="header" href="#false-dependencies-after-renaming" id="false-dependencies-after-renaming">False dependencies after renaming</a></h2>
<p>So does renaming actually remove our WAR and WAW dependencies? Yes! In the below
excerpt from the lectures we are shown a program that contains a set of
dependencies for each instruction. Because each instruction has these
dependencies, they must be executed in order, causing us to have a CPI of <code>1</code>.
However, with renaming, we are able to eliminate the WAR and WAW dependencies
improving our performance to a CPI of <code>0.33</code>.</p>
<p><img src="lesson6/./img/after-renaming.png" alt="after-renaming" /></p>
<h1><a class="header" href="#what-is-ilp" id="what-is-ilp">What is ILP?</a></h1>
<p><strong>Instruction level parallelism (ILP)</strong> is a property of a program given the
fact that it's running on an <em>ideal processor</em>. What's an ideal processor? An
ideal processor has these attributes:</p>
<ul>
<li>Processor dispatches an entire instruction in <code>1</code> cycle.</li>
<li>Processor can do any number of instructions in the same cycle.
<ul>
<li>The processor has to obey true dependencies when doing this.</li>
</ul>
</li>
</ul>
<p>So what are the steps to acquire the value of a program's ILP? A program's ILP
is equal to the IPC when executing on an ideal processor following the rules
stated above. Obviously ideal processors like this always aren't achievable, the
ILP for a program will be different on actual real-world processors. ILP gives
us a value, however, for the parallel nature of program - defining how many
true dependencies exist within the code.</p>
<p>The steps to acquire the ILP of a program are:</p>
<ol>
<li>Rename the registers, as shown previously.</li>
<li>&quot;Execute&quot; the code on the ideal processor.</li>
</ol>
<h2><a class="header" href="#ilp-example" id="ilp-example">ILP example</a></h2>
<p>The below excerpt from the lectures showcases how we can compute the ILP for a
given set of instructions: <code>ILP == num instructions/cycles</code>. In this example,
the professor identifies the true dependencies that exists for the set of
instructions and sees that the fifth instruction cannot execute until the writes
of instruction one and three are complete. Thus, this program will take 2 cycles
on an ideal processor to execute.</p>
<p>A neat trick that is described in the lecture is that we don't even have to
conduct register renaming in order to calculate the ILP. All we have to do is
identify the true dependencies - the register renaming will take care of the
false or name dependencies for us.</p>
<p><img src="lesson6/./img/ilp-example.png" alt="ilp-example" /></p>
<h2><a class="header" href="#ilp-quiz" id="ilp-quiz">ILP quiz</a></h2>
<p>Below is a quiz from the lectures showcase the technique for calculating ILP
described above. We don't conduct register renaming, we just identify the true
dependencies to determine on what cycle instructions are eligible to execute.
After we determine the minimum number of cycles to resolve the true
dependencies, we can find the ILP of the program.</p>
<p><img src="lesson6/./img/ilp-quiz.png" alt="ilp-quiz" /></p>
<h2><a class="header" href="#calculating-ilp-with-structural-and-control-dependencies" id="calculating-ilp-with-structural-and-control-dependencies">Calculating ILP with structural and control dependencies</a></h2>
<p>When calculating ILP, we don't account for <strong>structural dependencies</strong>. These
dependencies result for architectural issues like not having an adder available
for a specific operation, etc. Something outside of the programmer's control,
solely relies with the manufacturing of the processor.</p>
<p>For <strong>control dependencies</strong>, we essentially ignore them. We assume that we
have perfect same-cycle branch prediction. In the example below, we show that
a branch instruction has a data dependency on instructions prior to it, however,
because we have perfect branch prediction we know that we will jump to the
<code>Label</code>. Thus, we fetch the instructions from the <code>Label</code> and execute them, even
though our branch has not been executed, yet.</p>
<p><img src="lesson6/./img/ilp-w-control-struct-deps.png" alt="ilp-w-control-struct-deps" /></p>
<h2><a class="header" href="#ilp-vs-ipc" id="ilp-vs-ipc">ILP vs IPC</a></h2>
<p>Below is an excerpt from the class conducting a comparison of the calculations
for ILP vs IPC on a given example processor. The processor has some limitations
such as: 2-issue (2 instructions / cycle), 1 MUL and 1 ADD/SUB/XOR components
(structural limitations). With this, we calculate the ILP for the given
instructions and then we calculate the IPC twice: once ignoring the structural
limitations and once factoring in the structural limitations.</p>
<p>As we can see, the IPC for a processor can only be equal to or less than the
ILP. Real processors will have limitations and won't always be able to achieve
the ILP of a program.</p>
<p><img src="lesson6/./img/ilp-vs-ipc.png" alt="ilp-vs-ipc" /></p>
<h2><a class="header" href="#ilp-vs-ipc-quiz" id="ilp-vs-ipc-quiz">ILP vs IPC quiz</a></h2>
<p>Below is a quiz from the lecture that shows us how to calculate and compare the
ILP and IPC of a program for a given test processor. Note, the processor in this
example can only dispatch instructions <strong>in order</strong>, thus degrading our
performance in comparison with the out of order processor's we've seen in the
past. This limitation causes us to have to stop dispatching instructions when we
encounter a dependency.</p>
<p><img src="lesson6/./img/ilp-v-ipc-quiz.png" alt="ilp-v-ipc-quiz" /></p>
<h2><a class="header" href="#ilp-vs-ipc-discussion" id="ilp-vs-ipc-discussion">ILP vs IPC discussion</a></h2>
<p>This slide sorta restates what we covered earlier - a summary, essentially. The
ILP can only be calculated using the abstract concept of an ideal processor, and
it's generally representative of the best performance of a program. The ILP
will always be greater than or equal to the IPC, and the IPC is calculated using
an actual processor with issue and structural limitations.</p>
<p>The limitations for processors and how they affect IPC breaks down like this:</p>
<ul>
<li>If a processor <strong>narrowly issues</strong> instructions (1 - 2 instructions / cycle),
and only executes instructions <strong>in order</strong>, the processor will mostly be
limited by the <strong>narrowly issued</strong> attribute.</li>
<li>If a processor <strong>widely issues</strong> instructions (4 or more instructions / cycle)
, and only executes instructions <strong>in order</strong>, the processor will mostly be
limited by the <strong>in order</strong> attribute.</li>
<li>The <em>best</em> case is when the processor <strong>widely issues</strong> instructions but also
can execute instructions <strong>out of order</strong>. To do this, the processor will have
to dispatch a lot of instructions per cycle, eliminate false dependencies
through register renaming, and reorder instructions so they can be executed out
of order.</li>
</ul>
<p><img src="lesson6/./img/ilp-v-ipc-discussion.png" alt="ilp-v-ipc-discussion" /></p>
<h1><a class="header" href="#instruction-scheduling" id="instruction-scheduling">Instruction Scheduling</a></h1>
<p>This lesson covers how <strong>instruction scheduling</strong> allows us to execute programs
faster by dispatching more than one instruction per cycle while also handling
data dependencies.</p>
<h2><a class="header" href="#improving-ipc" id="improving-ipc">Improving IPC</a></h2>
<p>As we've seen in previous lectures, there are multiple ways for us to improve
IPC:</p>
<ul>
<li><strong>Handling control dependencies</strong> - if we have good <strong>branch prediction</strong> we
can better handle control dependencies, allowing us to fetch the correct
instructions and avoid wasting cycles.</li>
<li><strong>false data dependencies</strong> - when we encounter false or name dependencies
(WAR and WAW), we use <strong>register renaming</strong> to completely eliminate the hazards
posed by these data dependencies.</li>
<li><strong>true data dependencies</strong> - we can improve our performance even when RAW
dependencies are present using <strong>out-of-order</strong> execution. We find instructions
that are not dependent upon the writes of previous instructions and execute
those while we wait for the results of an instruction to be forwarded to
dependent instructions.</li>
<li><strong>structural dependencies</strong> - to improve our performance we must use wider
issuing processors - we'll be able to dispatch more instructions because we'll
have more resources available.</li>
</ul>
<h2><a class="header" href="#tomasulos-algorithm" id="tomasulos-algorithm">Tomasulo's algorithm</a></h2>
<p>Tomasulo's algorithm is the basis for modern processors' implementation of
out-of-order execution. It solves the requirements for <strong>register renaming</strong>,
and is able to determine which instructions have input ready and which
instructions are dependent upon pending writes. Below is an excerpt from the
class describing what the differences are between Tomasulo's algorithm and how
modern processors are designed.</p>
<p><img src="lesson7/./img/tomasulos-algorithm.png" alt="tomasulos-algorithm" /></p>
<h2><a class="header" href="#tomasulos-algorithm---the-big-picture" id="tomasulos-algorithm---the-big-picture">Tomasulo's algorithm - the big picture</a></h2>
<p>Below is an excerpt from the class drawing out the components that comprise
Tomasulo's algorithm. There are a lot of moving parts and they'll be broken
down into more detail later - right now we're just summarizing the pieces. Here
is a quick description of all of the parts:</p>
<ul>
<li><strong>instruction queue</strong> - the instruction queue is pretty self explanatory, this
is where instructions sit and wait to be <strong>issued</strong> - they are fetched from
memory.</li>
<li><strong>reservation station</strong> - Tomasulo's algorithm was designed for floating point
instructions, so this is where those types of instructions wait for their
required values before being <strong>dispatched</strong>. The reservation stations are split
into simple instructions that require and adder and more complex instructions
that might require a multiplier. Reservation stations enable Tomasulo's
algorithm to determine which instructions are ready to execute - implementing
out-of-order execution.</li>
<li><strong>registers</strong> - once instructions are dispatched from the reservation stations
, the values they generate are written (<strong>broadcasted</strong>) to the registers.
Instructions in other reservation stations will be able to acquire their
forwarded values so that they can be dispatched as well.</li>
</ul>
<p>On the left-hand side of this picture, you'll see that there's logic to cover
load and store instructions as well. These were not handled by Tomasulo's
algorithm, but modern processors now handle all instruction types similar to
Tomasulo's algorithm. The results of the load and store instructions are also
<strong>broadcasted</strong> so that other instructions waiting to be dispatched can take
advantage of the values before they are written.</p>
<p>There are three stages of Tomasulo's algorithm:</p>
<ul>
<li><strong>issue</strong> - this is when an instruction is sorted from the instruction queue
into either load / store or a floating point instruction.</li>
<li><strong>dispatch</strong> - this is when an instruction is actually executing using
processor resources.</li>
<li><strong>write result (broadcast)</strong> - this is when the result of the instruction is
written to the registers file for later use by dependent instructions.</li>
</ul>
<p><strong>Note</strong> - the instructions in the reservation stations also have two inputs
per station to receive broadcasts. This is to provide dependent instructions
with the necessary values to be dispatched as early as possible.</p>
<p><img src="lesson7/./img/the-big-picture.png" alt="the-big-picture" /></p>
<h1><a class="header" href="#issuing" id="issuing">Issuing</a></h1>
<p>Here are the steps that occur for <strong>issuing</strong> in Tomasulo's algorithm:</p>
<ul>
<li>Take the next instruction, in program order, from the instruction queue.
<ul>
<li>This must be done in program order in order for register renaming to work
correctly.</li>
</ul>
</li>
<li>Determine where input for an instruction originates.
<ul>
<li>Are the inputs required located in the register file? Or are we still
waiting on an instruction to generate the necessary values? If we need to wait
for an instruction, which one? This will utilize a RAT.</li>
</ul>
</li>
<li>Find a free reservation station of the correct type (addr or mult).
<ul>
<li>If all the reservation stations are busy, no instructions are issued this
cycle.</li>
</ul>
</li>
<li>Put the instruction in the reservation station.</li>
<li>Tag the destination register for the result of the instruction being issued.
<ul>
<li>This allows instructions who need this result to be able to reference the
correct name and identify their dependency.</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#issuing-example" id="issuing-example">Issuing example</a></h2>
<p>Below is an excerpt from the class that showcases how issuing works. Each
instruction is issued from the example instruction queue, and the instruction
queue is shifted when an instruction is successfully issued. The are plenty of
reservation stations available, so no instructions had to wait in order to
acquire a spot in a station.</p>
<p>Following the steps above, before an instruction is issued its inputs must be
translated using the RAT. If the value is available, the instruction will
read the input directly from the register file. The values that instructions
generate will be renamed and the RAT will contain the name of the reservation
station for generated values. This way, when other instructions that depend upon
a register value are issued, they will use the input generated from other
instructions by using their reservation station name.</p>
<p><img src="lesson7/./img/issue-example.png" alt="issue-example" /></p>
<h2><a class="header" href="#issue-quiz" id="issue-quiz">Issue quiz</a></h2>
<p>Below is a quiz from the lectures where we practice the issuing portion of
Tomasulo's algorithm. In this example, we are unable to issue the second
instruction because there are no spots left in the reservation stations. This
example is pretty straightforward.</p>
<p><img src="lesson7/./img/issue-quiz.png" alt="issue-quiz" /></p>
<h1><a class="header" href="#dispatching" id="dispatching">Dispatching</a></h1>
<p>Below is an excerpt from the class that showcases how dispatching works. The
results of an instruction from a reservation station is broadcast - its
generating reservation station number is attached to the value that it's
generating. Instructions that depend upon a value from that reservation station
are able to acquire the values necessary to be dispatched, and they will also
broadcast their results in the same manner.</p>
<p><img src="lesson7/./img/dispatch-example.png" alt="dispatch-example" /></p>
<h2><a class="header" href="#dispatching-when--1-instruction-is-ready" id="dispatching-when--1-instruction-is-ready">Dispatching when &gt; 1 instruction is ready</a></h2>
<p>Below is an excerpt from the lectures displaying a situation in which more than
one instruction in the reservation station is ready to execute. How do we go
about making the right decision for best performance? There are three options:</p>
<ul>
<li><strong>Dispatching the oldest instruction first</strong> - the most fair option and most
balanced for performance.</li>
<li><strong>Dispatching the instruction with the most dependencies</strong> - the highest
performance option, however, it requires us to search all reservation stations
and determine dependencies - power hungry.</li>
<li><strong>Randomly dispatching</strong> - not a bad option, and at some point every
instruction will be dispatched, but performance could be degraded.</li>
</ul>
<p><img src="lesson7/./img/dispatch-greater-than-1.png" alt="dispatch-greater-than-1" /></p>
<h2><a class="header" href="#dispatch-quiz" id="dispatch-quiz">Dispatch quiz</a></h2>
<p>Below is a quiz from the lectures on dispatching. The quiz asks why the
instruction held in <code>RS3</code> hasn't executed before the current state of the
reservation stations. Three options are given, but only two are reasonable.</p>
<ul>
<li>The first option: &quot;It was issued in the previous cycle&quot; is reasonable because
there can definitely be a time in which the instruction was issued too late so
it didn't have time to execute.</li>
<li>The second option: &quot;Another instruction was dispatched to the adder&quot; is
possible because maybe <code>RS1</code> was currently executing on the adder to generate a
value.</li>
<li>The third option: &quot;RS2 is older than RS3&quot; is not reasonable. The whole purpose
of Tomasulo's algorithm is to support out-of-order execution. Instructions will
be executed as soon as their values are available, despite how old an
instruction is.</li>
</ul>
<p><img src="lesson7/./img/dispatch-quiz.png" alt="dispatch-quiz" /></p>
<h1><a class="header" href="#writing-broadcast" id="writing-broadcast">Writing (Broadcast)</a></h1>
<p>Below is an excerpt from the lectures that shows us what happens when a result
is written or broadcasted to the reservation stations and the register file.
Some steps take place:</p>
<ul>
<li>The result is tagged with the reservation station that is generating this
result. This combo is then emitted onto the bus.</li>
<li>The result is written to the register file. We are able to determine what
register is being written based upon the RAT - it will contain the name of the
reservation station for the appropriate register.</li>
<li>The RAT is then updated, clearing the reservation station name from the table
and marking the location for a register as clear.</li>
<li>The spot the instruction held in the reservation station is freed.</li>
</ul>
<p><img src="lesson7/./img/write-result.png" alt="write-result" /></p>
<h2><a class="header" href="#more-than-1-broadcast" id="more-than-1-broadcast">More than 1 broadcast?</a></h2>
<p>There are multiple possibilities that we can consider when handling multiple
broadcasts. The first idea is to make more than one bus available - a bus for
each dispatch unit in the processor. While that sounds like a good idea, the
number of structural dependencies to implement this doubles for each bus -
that's a lot of resources and more logic is needed in order to correctly direct
the different values being generated.</p>
<p>Another option is to give priority to one dispatch unit over the other, and this
is the more likely case. In the example below, we give more priority to the
slower dispatch unit. Why? Because if one dispatch unit is slower, it's more
likely that faster dispatch units are waiting on values to be broadcasted by
the slower one.</p>
<p><img src="lesson7/./img/broadcast-greater-than-1.png" alt="broadcast-greater-than-1" /></p>
<h2><a class="header" href="#stale-broadcasts" id="stale-broadcasts">Stale broadcasts</a></h2>
<p>Can we have stale broadcasts? Is that really a thing? Not necessarily. In the
example below we see that a broadcast is occurring for <code>RS4</code>, but there's no
entry for that reservation station in the RAT. So where will this result be
written to - what's its destination?</p>
<p>We can see that <code>RS2</code> is generating a value and it has overwritten <code>F4</code>, which
was the original destination of <code>RS4</code>'s value - this is perfectly fine and works
as intended. <code>RS2</code> was the only instruction that depended on the output of
<code>RS4</code>, it will have the value it needs to update <code>F4</code>. All subsequent
instructions reading <code>F4</code> will utilize the value generated by <code>RS2</code> - the
&quot;stale&quot; value being broadcasted by <code>RS4</code> won't cause any issues.</p>
<p><img src="lesson7/./img/stale-broadcast.png" alt="stale-broadcast" /></p>
<h1><a class="header" href="#review" id="review">Review</a></h1>
<h2><a class="header" href="#part-1" id="part-1">Part 1</a></h2>
<p>Below is a summarization of Tomasulo's algorithm. In this algorithm, for each
instruction we are:</p>
<ul>
<li><strong>issuing</strong> - resolving the registers for an instruction using the RAT and
then updating the RAT with the reservation station name of an instruction
generating values.</li>
<li><strong>capturing</strong> - capturing output of instructions while in the reservation
station.</li>
<li><strong>dispatching</strong> - dispatching ready-to-execute instructions.</li>
<li><strong>writing results</strong> - broadcasting the results of instructions to instructions
capturing output in the reservation station, updating the register file and the
RAT.</li>
</ul>
<p>An awesome thing to note as that each of these things is happening for different
instructions on each cycle. Every instruction is in one of these stages as we execute instructions out of order.</p>
<p><img src="lesson7/./img/review-part-1.png" alt="review-part-1" /></p>
<h2><a class="header" href="#part-2" id="part-2">Part 2</a></h2>
<p>Below is an excerpt from the lecture that provides some further discussion of
Tomasulo's algorithm. Some questions that are posed:</p>
<ul>
<li><strong>Can we issue and dispatch an instruction in the same cycle?</strong> - no, we
cannot. During an issue and instruction is loaded into the reservation station,
we won't be able to dispatch it until we identify that it is ready for
executing. While its dependencies may be resolved when being entered into the
reservation station, we won't know it's ready to be dispatched until the next
cycle.</li>
<li><strong>Can we capture and dispatch an instruction in the same cycle?</strong> - also no,
for similar reasons as above. Once an instruction has captured all of its
necessary values, it still needs to be marked for execution after the
reservation stations have resolved which instructions are ready. This can happen
if hardware support is available.</li>
<li><strong>Can we update the RAT entry for an issue and a broadcast in the same
cycle?</strong> - yes, the broadcast and the issue can procedures can both update the
RAT in the same cycle, however, the issue will be the last to update the RAT.
This way, future instructions will reference the output of the reservation
station by the issue, not by the broadcast.</li>
</ul>
<p><img src="lesson7/./img/review-part-2.png" alt="review-part-2" /></p>
<h2><a class="header" href="#one-cycle-quiz-part-1" id="one-cycle-quiz-part-1">One cycle quiz part 1</a></h2>
<p>Below is a quiz from the lectures where we practice predicting what values will
exist in the register file and the RAT after one cycle takes place. In this
example we have a processor that does not allow issuing and dispatching in the
same cycle, but does allow capturing and dispatching and updating the RAT and
writing to the register file in the same cycle.</p>
<p>Given the values provided for the different parts of the scheduler, we are able
to determine that the <code>F1</code> entry in the RAT will be updated by the instruction
being issued, and the register file entry for <code>F0</code> will be updated by the
broadcast of <code>RS0</code>. Simultaneously, the entry for <code>F0</code> in the RAT will be
cleared.</p>
<p><img src="lesson7/./img/one-cycle-quiz-part-1.png" alt="one-cycle-quiz-part-1" /></p>
<h2><a class="header" href="#one-cycle-quiz-part-2" id="one-cycle-quiz-part-2">One cycle quiz part 2</a></h2>
<p>Below is a quiz from the lectures where we practice predicting what values will
exist in the reservation station for a cycle of the processor pipeline. We can
see that an instruction is issued, an instruction is freed from the reservation
station upon dispatch, the same instruction that was just issued is able to
capture the broadcast of the dispatched instruction.</p>
<p><img src="lesson7/./img/one-cycle-quiz-part-2.png" alt="one-cycle-quiz-part-2" /></p>
<h2><a class="header" href="#one-cycle-quiz-part-3" id="one-cycle-quiz-part-3">One cycle quiz part 3</a></h2>
<p>Below is a quiz from the lectures where we predict what instruction will be
dispatched from the reservation station in the example cycle. We can see that,
as soon as the instruction in <code>RS1</code> captures its required value, it will be
dispatched. The instruction in <code>RS2</code> will not be eligible for dispatch because
this processor does not allow issuing and dispatching in the same cycle.</p>
<p><img src="lesson7/./img/one-cycle-quiz-part-3.png" alt="one-cycle-quiz-part-3" /></p>
<h2><a class="header" href="#tomasulos-algorithm-quiz" id="tomasulos-algorithm-quiz">Tomasulo's algorithm quiz</a></h2>
<p>Below is a quiz from the lectures quizzing us on what is <strong>not</strong> true about
Tomasulo's algorithm. What's not true? Tomasulo's algorithm <strong>does not</strong>
dispatch instructions in program order and it <strong>does not</strong> write results in
program order. Tomasulo's algorithm <strong>does</strong> <em>issue</em> instructions in program
order.</p>
<p><img src="lesson7/./img/tomasulos-algorithm-quiz.png" alt="tomasulos-algorithm-quiz" /></p>
<h1><a class="header" href="#load-and-store" id="load-and-store">Load and Store</a></h1>
<p>Now that we've discussed how Tomasulo's algorithm handles data dependencies for
floating point instructions, how does it handle <strong>memory dependencies</strong>? Well
first, what memory dependencies are there? They're actually the same three
categories as data dependencies:</p>
<ul>
<li><strong>RAW</strong> - an instruction stores a word to some address and then a different
instruction loads a word from that same address - this is a true dependency.</li>
<li><strong>WAR</strong> - an instruction loads a word from memory and then another instruction
stores a word into memory; if these instructions got re-ordered, we could
possibly load the value from the store, thus giving us incorrect program
execution.</li>
<li><strong>WAW</strong> - two instructions both write to the same address; if these
instructions are re-ordered, it's possible that a stale value is written to the
address.</li>
</ul>
<p>So what do we do about this?:</p>
<ul>
<li>In Tomasulo's algorithm, we execute load and store instructions in order. Even
if a load instruction is ready to execute, it will be stalled until the store
instruction preceding it is completed - even if that store instruction is
stalled waiting on some value.</li>
<li>Modern processors identify dependencies, re-order load instructions, etc.
similar to how floating point instructions are handled by Tomasulo's algorithm.
These techniques will not be discussed in this lesson, however, they will be
touched upon later.</li>
</ul>
<h1><a class="header" href="#timing" id="timing">Timing</a></h1>
<p>Below is a quiz on timing from the lecture. Now that we understand the inner
working of Tomasulo's algorithm, we predict how long it takes for instructions
in the queue to execute based upon the rules provided.</p>
<p><img src="lesson7/./img/timing-quiz-part-1.png" alt="timing-quiz-part-1" /></p>
<h2><a class="header" href="#timing-quiz-part-2" id="timing-quiz-part-2">Timing quiz part 2</a></h2>
<p>Below is the second part of the previous quiz. Of note, instruction 5 is unable
to be dispatched until cycle 11 because the reservation stations are held by
instructions 3 and 4. Instruction 6 cannot be issued until instruction 4
completes its write back and is cleared from the reservation station.</p>
<p><img src="lesson7/./img/timing-quiz-part-2.png" alt="timing-quiz-part-2" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        
        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>
        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
