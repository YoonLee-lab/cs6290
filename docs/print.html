<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="welcome.html">Welcome</a></li><li class="chapter-item expanded "><a href="lesson1/introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="lesson2/metrics-and-evaluation.html"><strong aria-hidden="true">2.</strong> Metrics and Evaluation</a></li><li class="chapter-item expanded "><a href="lesson3/pipelining.html"><strong aria-hidden="true">3.</strong> Pipelining</a></li><li class="chapter-item expanded "><a href="lesson4/branches.html"><strong aria-hidden="true">4.</strong> Branches</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lesson4/prediction.html"><strong aria-hidden="true">4.1.</strong> Prediction</a></li><li class="chapter-item expanded "><a href="lesson4/branch-target-buffer.html"><strong aria-hidden="true">4.2.</strong> Branch Target Buffer</a></li><li class="chapter-item expanded "><a href="lesson4/direction-predictor.html"><strong aria-hidden="true">4.3.</strong> Direction Predictor</a></li><li class="chapter-item expanded "><a href="lesson4/2-bit-predictor.html"><strong aria-hidden="true">4.4.</strong> 2 Bit Predictor</a></li><li class="chapter-item expanded "><a href="lesson4/history-based-predictors.html"><strong aria-hidden="true">4.5.</strong> History Based Predictors</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#welcome" id="welcome">Welcome</a></h1>
<p>This notebook contains my personal notes for CS6290: High Performance Computer
Architecture offered at the Georgia Institute of Technology. A summary of the
course follows:</p>
<p>This course covers modern computer architecture, including branch prediction,
out-of-order instruction execution, cache optimizations, multi-level caches,
memory and storage, cache coherence and consistency, and multi- and many-core
processors.</p>
<h2><a class="header" href="#course-links" id="course-links">Course links</a></h2>
<ul>
<li><a href="https://omscs.gatech.edu/cs-6290-high-performance-computer-architecture">https://omscs.gatech.edu/cs-6290-high-performance-computer-architecture</a></li>
<li><a href="https://ebookcentral-proquest-com.prx.library.gatech.edu/lib/gatech/detail.action?docID=787253">https://ebookcentral-proquest-com.prx.library.gatech.edu/lib/gatech/detail.action?docID=787253</a></li>
</ul>
<h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<h2><a class="header" href="#what-is-computer-architecture" id="what-is-computer-architecture">What is computer architecture?</a></h2>
<p>We can think of computer architecture similar to how architecture is considered
for building. Buildings are designed to be well-suited for a specific purpose.
The same can be said of computers, as we design different computers for
different purposes. Some examples are desktop computers, laptops, and
cellphones; they all have different purposes and require different computer
architectures.</p>
<p><img src="lesson1/./img/what-is-computer-architecture.png" alt="what-is-computer-architecture" /></p>
<h2><a class="header" href="#why-do-we-need-computer-architecture" id="why-do-we-need-computer-architecture">Why do we need computer architecture?</a></h2>
<ol>
<li>To improve performance based upon some specific measure. These measures
could include:</li>
</ol>
<ul>
<li>Speed</li>
<li>Battery life</li>
<li>Size</li>
<li>Weight</li>
<li>Energy efficiency, etc.</li>
</ul>
<ol start="2">
<li>To improve the abilities provided by a computer. These abilities could include:</li>
</ol>
<ul>
<li>3D graphics</li>
<li>Debugging support</li>
<li>Security, etc.</li>
</ul>
<p>The first need for computer architecture is about making computers cheaper,
faster, smaller, etc. while the second need is based around making new things
possible/providing new functionality. Computer architecture utilizes discoveries
in fabrication technology and circuit design to achieve the goals stated above.</p>
<p><img src="lesson1/./img/why-do-we-need-computer-architecture.png" alt="why-do-we-need-computer-architecture" /></p>
<h2><a class="header" href="#computer-architecture-and-technology-trends" id="computer-architecture-and-technology-trends">Computer architecture and technology trends</a></h2>
<p>Computer architecture is about building future computers. The progress in
computer manufacturing technology is fast. We shouldn't design a new computer
with current technology and parts. By the time we've designed our new computer,
it's obsolete and using old technology. We need to track technology trends in
order to predict what's available in the future, allowing us to better design a
new computer using these new technologies.</p>
<p><img src="lesson1/./img/computer-arch-tech-trends.png" alt="computer-arch-tech-trends" /></p>
<h2><a class="header" href="#moores-law" id="moores-law">Moore's law</a></h2>
<p>Moore's law provides a good starting point for predicting what future computers
will look like based upon what is currently available today. As computer
architects, we can use these predictions to guide our expectations of what
technologies will be available for use when designing future computers. Below is
a high-level summary of Moore's law:</p>
<p><img src="lesson1/./img/moores-law.png" alt="moores-law" /></p>
<h2><a class="header" href="#the-memory-wall" id="the-memory-wall">The memory wall</a></h2>
<p>Processor speed doubles almost every two years, while memory capacity also
experiences the same phenomenon. Memory latency, however, has not kept pace with
processor speed and memory capacity. This is what's called the memory wall and,
in order to mitigate these differences in speed when processors need to access
memory, we have been using caches to close that gap. Caches can be thought of as
a series of stairs for memory access speed, and cache misses are the base of the
staircase with the slowest memory access speed possible. A high-level
illustration of the memory wall trend is provided below:</p>
<p><img src="lesson1/./img/memory-wall.png" alt="memory-wall" /></p>
<h2><a class="header" href="#power-consumption" id="power-consumption">Power consumption</a></h2>
<p>There are two kinds of power that a processor consumes:</p>
<ul>
<li>Dynamic (active) power - consumed by activity in a circuit</li>
<li>Static power - consumed when powered on but idle</li>
</ul>
<h3><a class="header" href="#active-power" id="active-power">Active power</a></h3>
<p>Below is an illustrative representation of the equation for calculating active
power consumption by a processor. The representation also displays how we can
calculate the change in active power consumption when a different chip
configuration is used, and if voltage and chip frequency are changed. An
explanation of each variable in the active power equation follows:</p>
<ul>
<li>capacitance - roughly proportional to chip area / larger chips will have more
capacitance</li>
<li>voltage - quadratic relationship between voltage and power consumption</li>
<li>frequency - clock frequency (GHz) of a processor</li>
<li>alpha - activity factor (percentage of processor transistors active for any
given clock cycle)</li>
</ul>
<p><img src="lesson1/./img/active-power.png" alt="active-power" /></p>
<h3><a class="header" href="#static-power" id="static-power">Static power</a></h3>
<p>Static power is power consumed while the processor is idle. Some of this is due
to the voltage being too low to prevent transistors from leaking/wasting energy,
but it can also come from other sources as well. Below is a high-level
representation identifying the relationship between static and active power as
voltage increases or decreases in the circuit. This representation also
identifies that there is some optimal voltage settings for circuits to avoid
utilize too much power in bot the active and static states.</p>
<p><img src="lesson1/./img/static-power.png" alt="static-power" /></p>
<h2><a class="header" href="#fabrication-cost-and-yield" id="fabrication-cost-and-yield">Fabrication cost and yield</a></h2>
<p>The cost to manufacture and convert silicone wafers to useable computer chips
is pretty much static. We, primarily, have to account for chip yield when a
silicone wafer is divided into a number computer chips - some of these chips
can have defects while others work fine. Below is high-level representation of
the silicone wafer to computer chip manufacturing process, and how the yield
is derived based upon the number of defects in wafer versus the number of chips
extracted from a wafer.</p>
<p><img src="lesson1/./img/fab-yield.png" alt="fab-yield" /></p>
<p>Below is an example of how we can calculate the fabrication cost per chip
based upon chip size. Smaller chips cost less to manufacture, while larger chips
cost the most. Smaller chips, over time according to Moore's Law, will be able
to do more for less as we are able to fit smaller transistors onto the chips.
Larger chips will be able to remain the same size and cost, however, they will
be faster overall due to Moore's Law.</p>
<p><img src="lesson1/./img/fab-cost.png" alt="fab-cost" /></p>
<h2><a class="header" href="#references" id="references">References</a></h2>
<ol>
<li><a href="lesson1/./pdf/Lesson1Notes.pdf">Lesson 1 Notes</a></li>
</ol>
<h1><a class="header" href="#metrics-and-evaluation" id="metrics-and-evaluation">Metrics and Evaluation</a></h1>
<p>This lesson covers latency and throughput, two metrics used to measure computer
performance.</p>
<h2><a class="header" href="#performance" id="performance">Performance</a></h2>
<ul>
<li><strong>Latency</strong> - the total time it takes for an operation to complete, from start
to finish.</li>
<li><strong>Throughput</strong> - <em>not the inverse of latency</em>; because operations can take
place concurrently within a pipeline, this can be thought of as the number of
operations completed per unit measure of time.</li>
</ul>
<p>Below is a high-level representation of this concept.</p>
<p><img src="lesson2/./img/latency-throughput.png" alt="latency-throughput" /></p>
<h2><a class="header" href="#comparing-performance" id="comparing-performance">Comparing performance</a></h2>
<p>Knowing how to measure performance, we can now compare the performance of two
systems using our metrics. We want to be able to make a definitive statement
about the <strong>speedup</strong> of a system in comparison to another system, basically
stating that system <strong>x</strong> is faster than system <strong>y</strong>. We can compute this with
both <strong>latency</strong> and <strong>throughput</strong>, however, our equations are slightly
different. Below is an image showing us the equations for computing <strong>speedup</strong>
using our two metrics.</p>
<p><img src="lesson2/./img/comparing-performance.png" alt="comparing-performance" /></p>
<h2><a class="header" href="#speedup" id="speedup">Speedup</a></h2>
<p>A <strong>speedup</strong> value larger than 1 means we have improved performance. With
improved performance, we achieve higher throughput and shorter execution time. A
speedup less than 1 means that we have worse performance. When computing
speedup, we need to remember that performance is directly proportional to
throughput and performance has an inverse relationship with latency.</p>
<p><img src="lesson2/./img/speedup.png" alt="speedup" /></p>
<h2><a class="header" href="#measuring-performance" id="measuring-performance">Measuring performance</a></h2>
<p>What workload do we use to measure performance of different systems? We can't
use an actual user workload because:</p>
<ul>
<li>Many different users use computers in many different ways with many different
programs.</li>
<li>One workload will not be representative of all users.</li>
<li>How are we supposed to acquire the workload data?</li>
</ul>
<p>In order to solve this issue, we use <strong>benchmark</strong> workloads.</p>
<h2><a class="header" href="#benchmarks" id="benchmarks">Benchmarks</a></h2>
<p>Benchmarks are programs and input data that users and organizations have agreed
upon for use in performance measurements. Usually, we don't have just one
benchmark program but a <strong>benchmark suite</strong> consisting of multiple programs and
input data. Each program within a benchmark suite is representative of a type
of application.</p>
<h2><a class="header" href="#types-of-benchmarks" id="types-of-benchmarks">Types of benchmarks</a></h2>
<p>So what types of benchmarks are commonly used to measure performance? We have:</p>
<ul>
<li>
<p><strong>Real applications</strong></p>
<ul>
<li>Most representative of real workloads.</li>
<li>Also the most difficult to setup on new machines. Our testing environment
likely doesn't have an operating system, hardware, graphics processors, etc.</li>
</ul>
</li>
<li>
<p><strong>Kernels</strong></p>
<ul>
<li>The most time consuming portions of an application, usually a loop of some
sort. We've isolated these processing intensive sections of code to test our
machine. <em>Usually good for testing prototypes.</em></li>
</ul>
</li>
<li>
<p><strong>Synthetic benchmarks</strong></p>
<ul>
<li>Behave similar to kernels but are simpler to compile. We utilize these
benchmarks when testing early prototypes of a machine. <em>Usually good for design</em>
<em>studies.</em></li>
</ul>
</li>
<li>
<p><strong>Peak performance</strong></p>
<ul>
<li>Performance that's not based on running against actual code. The theoretical
highest number of instructions per second. <em>Usually good for marketing.</em></li>
</ul>
</li>
</ul>
<h2><a class="header" href="#benchmark-standard" id="benchmark-standard">Benchmark standard</a></h2>
<p>So how are benchmark suites created? Who makes them and what are the standards?
There exist standards organizations that receive input from manufacturers, user
groups, and experts in academia and these organizations produce standard
benchmark suites. Some well known standard benchmark suites are:</p>
<ul>
<li><strong>TPC</strong> - Benchmarks used for databases, web servers, data mining, and other
transaction processing. <a href="lesson2/metrics-and-evaluation.html#references">[1]</a></li>
<li><strong>EEMBC</strong> - Used for embedded processing. <a href="lesson2/metrics-and-evaluation.html#preferences">[2]</a></li>
<li><strong>SPEC</strong> - Used to evaluate engineering work stations and raw processors. SPEC
encompasses a large set of workloads, trying to cover a variety of uses for
processors in high performance systems. A breakdown of these workloads is in the
image below. <a href="lesson2/metrics-and-evaluation.html#preferences">[3]</a></li>
</ul>
<p><img src="lesson2/./img/benchmark-standard.png" alt="benchmark-standard" /></p>
<h2><a class="header" href="#summarizing-performance" id="summarizing-performance">Summarizing performance</a></h2>
<p>To summarize performance, we are looking for the <strong>average execution time</strong>.
A demonstration on how to calculate this is shown below. We should refrain from
averaging the speedups for each application tested - averaging ratios will not
provide useable data to summarize performance. In order to acquire the average
speedup, we need to use the <strong>geometric mean</strong> for the execution times of each
computer.</p>
<p><img src="lesson2/./img/summarizing-performance.png" alt="summarizing-performance" /></p>
<h2><a class="header" href="#iron-law-of-performance" id="iron-law-of-performance">Iron Law of performance</a></h2>
<p>Processor time (CPU time) can be expressed as:
<code>(instructions/program) * (cycles/instruction) * (seconds/cycles)</code></p>
<p>So why do we think about these components of processor time instead of just
measuring processor time directly? These three components allow us to think
about the different aspects of computer architecture.</p>
<ul>
<li><code>(instructions/program)</code> - influenced by the algorithm used to create the
program, the compiler used to interpret and generate the program, and the
instruction set being used.</li>
<li><code>(cycles/instruction)</code> - influenced by the instruction set being used, and
the processor design.</li>
<li><code>(seconds/cycle)</code> - influenced by the processor design, circuit design, and
transistor physics.</li>
</ul>
<p>Computer architecture primarily focuses on instruction set and processor design,
and good designs of these two aspects attempts to balance their effects on
CPU time.</p>
<p><img src="lesson2/./img/iron-law.png" alt="iron-law" /></p>
<h2><a class="header" href="#iron-law-for-unequal-instruction-times" id="iron-law-for-unequal-instruction-times">Iron Law for unequal instruction times</a></h2>
<p>It's pretty simple to calculate CPU times when we assume that all instructions
for a program will take the same amount of cycles to execute. This isn't always
the case, however. We need to be able to sum all the cycles for each type of
instruction before we multiply this with our time per cycle. Below is a
representation of this concept, showing us how to calculate a more realistic
CPU time.</p>
<p><img src="lesson2/./img/iron-law-unequal.png" alt="iron-law-unequal" /></p>
<p>Below is an example problem in which we calculate the sum of cycles for a
program in order to determine the CPU time.</p>
<p><img src="lesson2/./img/iron-law-quiz.png" alt="iron-law-quiz" /></p>
<h2><a class="header" href="#amdahls-law" id="amdahls-law">Amdahl's Law</a></h2>
<p>This law is useful when we need to calculate the overall speedup of the entire
program, but we only enhanced a fraction of the program. Below is an image
that attempts to explain the equation for Amdahl's Law, a description of each
variable in the equation is provided:</p>
<ul>
<li><code>(1 - frac_enh)</code> - the fraction of the program that wasn't enhanced.</li>
<li><code>frac_enh</code> - the fraction of the program that was enhanced.</li>
<li><code>speedup_enh</code> - the speedup achieved due to the enhancement.</li>
</ul>
<p>It's very important to understand that <code>frac_enh</code> is a percentage of the
original execution time that is affected by the enhancement.</p>
<p><img src="lesson2/./img/amdahls-law.png" alt="amdahls-law" /></p>
<h3><a class="header" href="#implications" id="implications">Implications</a></h3>
<p>It's important to aim for enhancements that achieve a speedup on a larger
percentage of execution time for a program. This is demonstrated mathematically
below.</p>
<p><img src="lesson2/./img/amdahls-law-implications.png" alt="amdahls-law-implications" /></p>
<p>Below is an example of how to use Amdahl's Law to compare multiple possible
improvements.</p>
<p><img src="lesson2/./img/amdahls-law-quiz.png" alt="amdahls-law-quiz" /></p>
<h2><a class="header" href="#lhadmas-law" id="lhadmas-law">Lhadma's Law</a></h2>
<p>This law is jokingly used to express the opposite of Amdahl's Law. While
Amdahl's Law says to optimize for the common case, Lhadma's warns us that we
should avoid attempting to optimize too much at the expense of other parts of
our performance. An example is provided below.</p>
<p><img src="lesson2/./img/lhadmas-law.png" alt="lhadmas-law" /></p>
<h2><a class="header" href="#diminishing-returns" id="diminishing-returns">Diminishing returns</a></h2>
<p>This concept covers the idea that as computer architects, we need to
continuously review what needs to be enhanced within a system instead of
continuously enhancing the same portion of a system. This concept stems from
Amdahl's Law, as it describes that, eventually, the enhanced portion of a
system will become smaller as we apply enhancements across generations. We will
achieve a diminished speedup if we continue to enhance the same portion of the
system. We need to continually reassess what is the common case when conducting
our enhancements. Below is a high level representation of this idea.</p>
<p><img src="lesson2/./img/diminishing-returns.png" alt="diminishing-returns" /></p>
<h2><a class="header" href="#references-1" id="references-1">References</a></h2>
<ol>
<li><a href="http://www.tpc.org/tpch/">http://www.tpc.org/tpch/</a></li>
<li><a href="https://www.eembc.org/">https://www.eembc.org/</a></li>
<li><a href="https://www.spec.org/benchmarks.html">https://www.spec.org/benchmarks.html</a></li>
<li><a href="lesson2/./pdf/Lesson2Notes.pdf">Lesson 2 Notes</a></li>
</ol>
<h1><a class="header" href="#pipelining" id="pipelining">Pipelining</a></h1>
<p>This lesson reviews pipelining to set the stage for more advanced topics.</p>
<h2><a class="header" href="#pipelining-in-a-processor" id="pipelining-in-a-processor">Pipelining in a processor</a></h2>
<p>This section covers basic pipelining in a processor. Most processors are much
more complex than the example provided here, however, this is used to review
content for students.</p>
<p>In a traditional processor pipeline, we have are series of stages. The following
listing of stages is not <em>all</em> stages, but it encompasses the important ones:</p>
<ul>
<li><strong>fetch</strong></li>
<li><strong>read</strong></li>
<li><strong>decode</strong></li>
<li><strong>execute</strong></li>
<li><strong>memory access</strong></li>
<li><strong>write</strong></li>
</ul>
<p>So how does pipelining apply to these stages? Instead of fetching, decoding, and
executing one instruction at a time, while one instruction is being decoded,
another instruction can be fetched from instruction memory. Then, when one
instruction is being executed, we can be decoding the instruction behind it.
So, while the latency may not change, the throughput of instructions through
the pipeline increases. Below is a high level representation of this concept:</p>
<p><img src="lesson3/./img/pipelining-in-processor.png" alt="pipeline-in-processor" /></p>
<p>Below is an example of calculating the latency of process with and without a
pipeline.</p>
<p><img src="lesson3/./img/laundry-pipelining.png" alt="laundry-pipelining" /></p>
<p>Below is a similar example as the one above, however, this one applies to
instructions and cycles.</p>
<p><img src="lesson3/./img/instruction-pipelining.png" alt="instruction-pipelining" /></p>
<h2><a class="header" href="#pipeline-cycles-per-instruction" id="pipeline-cycles-per-instruction">Pipeline cycles per instruction</a></h2>
<p>Throughout these notes we've been assuming one cycle per instruction, or a CPI
of 1, when our pipeline is full. In the real-world, however, we'll have billions
of instructions to execute - will our CPI always be 1? Here are some reasons
why our CPI might not be 1:</p>
<ul>
<li><strong>initial fill</strong> - when the pipeline initially fills up, our CPI will not be
equal to 1. Regardless, as our instruction number reaches infinity, CPI will
begin to approach 1.</li>
<li><strong>pipeline stalls</strong> - there exists the possibility that a fault occurs in the
pipeline and an instruction stalls, causing it to have to remain at that stage
for a cycle.</li>
</ul>
<p>Below is a high-level representation of how a CPI can be greater than 1.</p>
<p><img src="lesson3/./img/pipeline-cpi.png" alt="pipeline-cpi" /></p>
<h2><a class="header" href="#processor-pipeline-stalls" id="processor-pipeline-stalls">Processor pipeline stalls</a></h2>
<p>A processor pipeline stall usually occurs when some instructions depend upon the
outcome of previous instructions that conduct a read/write. In the example
below, the program loads a value into a register, increments the register, and
then stores that value into a different register. The load operation must occur
before the increment and load instruction, otherwise the increment instruction
will be incrementing an incorrect value. Because of this dependency, a processor
pipeline stall occurs, and a <strong>bubble</strong> in the pipeline is created. The
increment instruction must wait two cycles until the memory is read and written
into the register that is to be incremented.</p>
<p>This phenomenon causes our CPI to be greater than 1.</p>
<p><img src="lesson3/./img/processor-pipeline-stalls.png" alt="processor-pipeline-stalls" /></p>
<h2><a class="header" href="#processor-pipeline-stalls-and-flushes" id="processor-pipeline-stalls-and-flushes">Processor pipeline stalls and flushes</a></h2>
<p>A processor pipeline flush occurs when the processor pipeline fetches and
decodes instructions that aren't actually supposed to be executed, so they're
removed from the pipeline and replaced with bubbles. Below is an example
demonstrating what happens when a <code>JMP</code> instruction is introduced into the
pipeline. Some instructions behind the <code>JMP</code> are fetched and decoded, however,
they are fetched from an incorrect location in memory. After the <code>ALU</code>
determines the destination of the <code>JMP</code>, instructions from the <code>JMP</code> destination
are fetched and decoded, and the instructions that weren't destined to be
executed are flushed from the pipeline.</p>
<p>This is another phenomenon that could cause the CPI to be larger than 1.</p>
<p><img src="lesson3/./img/processor-pipeline-flushes.png" alt="processor-pipeline-flushes" /></p>
<h2><a class="header" href="#control-dependencies" id="control-dependencies">Control dependencies</a></h2>
<p>The problems described in the previous sections that cause these processor
pipeline stalls are called <strong>control dependencies</strong>. The example provided below
provides a high-level representation of a control dependency. In the scenario,
a branch instruction will jump to some <strong>label</strong> in code, however, to sections
of code depend upon the branch: the code directly after the branch and the code
contained at the label.</p>
<p>This example also shows us how to predict the CPI based upon this concept of
control dependencies. Given a percentage of instructions are branches, given
that a percentages of branches are actually taken, and given that we know that
the fetching and decoding of control dependencies causes at least two bubbles in
the pipeline, we can calculate the increase in CPI from its normal value of 1.</p>
<p>In later discussions, we will cover a concept called <strong>branch prediction</strong> that
is designed to mitigate the occurrence of these bubbles within the pipeline by
predicting where a branch will land in order to fetch and decode the correct
instructions. Control dependencies can cause even more bubbles to form in a
pipeline if the pipeline contains more than 5 stages, so these issues definitely
need to be mitigated to increase performance and normalize the CPI.</p>
<p><img src="lesson3/./img/control-dependencies.png" alt="control-dependencies" /></p>
<p>Below is an example question from the lectures that asks us to determine the CPI
given a percentage of branches/jumps taken and when the branches/jumps are
computed in the pipeline.</p>
<p><img src="lesson3/./img/control-dependencies-quiz.png" alt="control-dependencies-quiz" /></p>
<h2><a class="header" href="#data-dependencies" id="data-dependencies">Data dependencies</a></h2>
<p>Briefly described in <a href="lesson3/pipelining.html#processor-pipeline-stalls">this</a> section,
<strong>data dependencies</strong> occur when one instruction depends upon the outcome of
another instruction. TODO types of dependencies exist:</p>
<ul>
<li><strong>read after write (RAW)</strong> - type of dependency in which one instruction
relies upon the previous instruction writing some data that will be used by the
dependent instruction. This type of dependency is also called a <strong>flow</strong>
dependence because the data flows from one instruction to the other. This type
of dependency is also called a <strong>true</strong> dependency because the value being used
by the dependent instruction doesn't exist until the previous instruction
writes.</li>
<li><strong>write after write (WAW)</strong> - type of dependency in which the order of write
operations needs to be preserved. The example provided uses registers, in which
a specific value is expected to be within <code>R1</code> for future instructions. These
writes will not be able to to be conducted out of order - they both write to
the same location. This type of dependency is also called an <strong>output</strong>
dependency.</li>
<li><strong>write after read (WAR)</strong> - type of dependency in which a previous
instruction needs to read some data, but a future instruction intends to write
to that data. These instructions can not be interleaved, the previous
instruction expects the data to be unchanged upon read. The read will take place
before the write, creating the dependency. This type of dependency is also
called an <strong>anti-dependency</strong> because it reversed the <strong>RAW</strong> dependency.</li>
</ul>
<p>The <strong>WAW</strong> and <strong>WAR</strong> dependencies are called  <strong>false</strong> or <strong>named</strong>
dependencies. <strong>Read after read (RAR)</strong> is not a dependency.</p>
<p><img src="lesson3/./img/data-dependencies.png" alt="data-dependencies" /></p>
<p>Below is an example from the lectures inspecting a series of instructions to
determine what data dependencies exist.</p>
<p><img src="lesson3/./img/data-dependencies-quiz.png" alt="data-dependencies-quiz" /></p>
<h2><a class="header" href="#data-dependencies-and-hazards" id="data-dependencies-and-hazards">Data dependencies and hazards</a></h2>
<p>A <strong>hazard</strong> is when a dependence results in incorrect execution of an
instruction. In the example provided below, there are three instructions that
have dependencies but their dependencies will not result in incorrect values
being used for execution.</p>
<p>The <code>DIV</code> instruction, however, will be using a <strong>stale</strong> value for <code>R4</code> when it
executes and writes to <code>R10</code> because, when the <code>DIV</code> instruction reaches the
<strong>decode</strong> stage of the pipeline, the <code>SUB</code> instruction has not yet written its
value to <code>R4</code>.</p>
<p>Hazards can both be a property of the program as well as because of the
pipeline. Another example provided in the image below shows that, in this 5
stage pipeline, true dependencies do not create a hazard when 3 or more
instructions separate the dependent instructions. This is because, by the time
the first instruction in the dependent pair executes and writes, the second
dependent instruction is still being fetched.</p>
<p><img src="lesson3/./img/hazards.png" alt="hazards" /></p>
<p>Below is an example problem with a 3 stage pipeline, demonstrating how we can
inspect instructions to determine dependencies and hazards.</p>
<p><img src="lesson3/./img/hazards-quiz.png" alt="hazards-quiz" /></p>
<h2><a class="header" href="#handling-of-hazards" id="handling-of-hazards">Handling of hazards</a></h2>
<p>We need to introduce mechanisms to handle hazards we detect in order to protect
our CPI and performance. We don't care about all dependencies that are
introduced, only the ones we know will cause incorrect execution. These are our
possible mitigation techniques:</p>
<ul>
<li><strong>Flush dependent instructions</strong> - used for control dependencies. We don't
intend to execute instructions introduced into the pipeline by control
dependencies.</li>
<li><strong>Stall dependent instructions</strong> - used for data dependencies in order to
prevent instructions from reading invalid values.</li>
<li><strong>Fix values read by dependent instructions</strong> - also used for data
dependencies, this introduces the concept of <strong>forwarding</strong>, providing the a
dependent instruction with the value it needs to correctly decode and execute.</li>
</ul>
<p>Below is a high-level representation of these hazard handling mechanisms.</p>
<p><img src="lesson3/./img/handling-hazards.png" alt="handling-hazards" /></p>
<p>Below is an example problem with a 5 stage pipeline and a series of instructions
containing multiple dependencies and hazards that must be avoided. This example
demonstrates how we can use the pipeline to determine when it is appropriate to
flush, stall, or forward to handle hazards.</p>
<p><img src="lesson3/./img/handling-hazards-quiz.png" alt="handling-hazards-quiz" /></p>
<h2><a class="header" href="#how-many-stages" id="how-many-stages">How many stages?</a></h2>
<p>In the pipelines we've reviewed, the ideal CPI is 1. Later in the course,
pipelines with a CPI higher than 1 are expected because the pipeline is
attempting to execute more than one instruction per cycle. Regardless, each
pipeline setup has an ideal CPI that it attempts to achieve.</p>
<p>So what happens if we add more stages? Well, we get more hazards. If a branch
is resolved in the third cycle of a pipeline, we only have to flush the two
previous instructions that were fetched and decoded. If a branch is resolved
in, for example, the tenth cycle of a pipeline, now 9 instructions have to be
flushed from the pipeline - kinda wasteful. With more hazards, our CPI also
increases.</p>
<p>Inversely, with more stages in our pipeline there's less work being done per
stage, decreasing our cycle time - we can execute cycles faster. Remember the
Iron Law?</p>
<p><code>CPU time = #instructions * CPI * cycle_time</code></p>
<p>If our number of instructions stays the same, but our CPI increases and our
cycle time decreases, we achieve a balance even with this longer pipeline. We
carefully choose the number of stages in our pipeline to balance the
relationship between CPI and cycle time.</p>
<p>Modern processors achieve the most performance with a processor pipeline of
30 - 40 stages as this strikes a perfect balance between cycle time and CPI.
This is great, but with an increase in number or processor pipeline stages, we
also draw a lot of power because we execute a lot of cycles per second. Thus,
a reasonable number of stages for the processor pipeline of modern processors is
10 - 15 stages as this strikes the best balance between performance and power.</p>
<p>Below is a high-level representation of the concepts described above.</p>
<p><img src="lesson3/./img/how-many-stages.png" alt="how-many-stages" /></p>
<h2><a class="header" href="#references-2" id="references-2">References</a></h2>
<ol>
<li><a href="lesson3/./pdf/Lesson3Notes.pdf">Lesson 3 Notes</a></li>
</ol>
<h1><a class="header" href="#branches" id="branches">Branches</a></h1>
<p>In the previous <a href="lesson4/../lesson3/pipelining.html">pipelining</a> lesson, we saw the
effects that hazards have on performance. Branches and jumps are common
instructions that introduce control dependencies - our only solution can't just
be to flush the processor pipeline each time a branch or jump is decoded. This
lesson covers techniques used to effectively avoid hazards introduced by control
dependencies.</p>
<h2><a class="header" href="#branch-in-a-pipeline" id="branch-in-a-pipeline">Branch in a pipeline</a></h2>
<p>The example provided below uses a 5 stage processor pipeline to demonstrate the
costs of incorrect branch prediction. The example explains how a branch
instruction works: is registers <code>R1</code> and <code>R2</code> are equal, the immediate value
represented by <code>Label</code> will be added to the program counter, <code>PC</code>, and execution
will begin at that location in memory. Otherwise, the program counter will be
incremented regularly - <code>R1</code> and <code>R2</code> are not equal.</p>
<p>In this scenario, a branch instruction enters the pipeline at cycle 1. The
branch instruction isn't evaluated until cycle 3 by the ALU. Meanwhile, two
instructions are fetched and decoded - these are represented by a <code>??</code> because
the instructions can be fetched either from memory directly after the branch
instruction or the landing location in memory if the branch instruction is
taken.</p>
<p>If we predict the outcome of the branch instruction correctly, the instructions
that were fetched and decoded will execute immediately after the branch
instruction leaves the pipeline - the branch will have taken 1 cycle to
complete. If we predicted the outcome of the branch incorrectly and loaded the
wrong instructions into the pipeline, these instructions will have to be flushed
and the branch will have taken 3 cycles to complete.</p>
<p><img src="lesson4/./img/branch-in-a-pipeline.png" alt="branch-in-a-pipeline" /></p>
<h2><a class="header" href="#references-3" id="references-3">References</a></h2>
<ol>
<li><a href="lesson4/./pdf/Lesson4Notes.pdf">Lesson 4 Notes</a></li>
</ol>
<h1><a class="header" href="#prediction" id="prediction">Prediction</a></h1>
<h2><a class="header" href="#branch-prediction-requirements" id="branch-prediction-requirements">Branch prediction requirements</a></h2>
<p>What do we need in order to successfully predict whether a branch will be taken
or not? What do need in order to determine where the branch is going if it's
taken?</p>
<p>The requirements are as follows:</p>
<ul>
<li>Branch prediction needs to work using only the knowledge of where we fetch the
current instruction from.
<ul>
<li>We need branch prediction to guess the program counter of the next
instruction to fetch.</li>
</ul>
</li>
<li>With branch prediction we must correctly guess:
<ul>
<li>Is this a branch?</li>
<li>If it is a branch, is it taken?</li>
<li>If it is a taken branch, what is the target program counter?</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#branch-prediction-accuracy" id="branch-prediction-accuracy">Branch prediction accuracy</a></h2>
<p>The image below demonstrates how we can calculate the theoretical CPI of a
processor using branch prediction given its branch prediction accuracy,
penalty per missed prediction, and percentage of instructions that constitute
branches within a benchmark program. The equation is contains these components:</p>
<ul>
<li><code>base CPI</code> - in the example below, this is <code>1</code>.</li>
<li><code>predictor accuracy</code> - this is the number of mis-predicted branches per
instruction.</li>
<li><code>penalty incurred</code> - the penalty is dictated by the number of stages in a
pipeline and when the branch instruction is evaluated.</li>
</ul>
<p>A more accurate branch predictor increases our performance, decreasing the
number of penalties taken for mis-predicted branches, thus decreasing our CPI.
The amount of help a better predictor has changes depending upon how long the
processor pipeline is. Longer pipelines benefit more from better branch
prediction than shorter ones.</p>
<p><img src="lesson4/./img/branch-prediction-accuracy.png" alt="branch-prediction-accuracy" /></p>
<p>Below is branch prediction example problem. The key to this problem is the fact
that no branch prediction is being conducted so, until each instruction is
decoded, another instruction will not be fetched, thus creating a bubble within
the pipeline. So, non-branch instructions will take 2 cycles to execute and
branch instructions will take 3 cycles to execute. Instructions will not be
fetched after a branch instruction is decoded because the branch instruction
still has to be evaluated prior to fetching from the possible branch
destination.</p>
<p>In contrast, each instruction for the perfect branch predictor takes 1
instruction, no bubbles exist within the processor pipeline. This is because
the branch predictor knows which instruction will be fetched next before the
previous one is decoded - including the branches.</p>
<p><img src="lesson4/./img/branch-prediction-benefit-quiz.png" alt="branch-prediction-benefit-quiz" /></p>
<h2><a class="header" href="#performance-with-not-taken-prediction" id="performance-with-not-taken-prediction">Performance with not-taken prediction</a></h2>
<p>A simple implementation of branch prediction that has an increase in performance
over just not predicting branches at all is the prediction that all branches
are <strong>not taken</strong>. If no predictions are made, ever, and instructions are only
fetched after previous instructions are decoded, branches will always take 3
cycles in our example 5 stage pipeline and regular instructions will take 2
cycles.</p>
<p>If we always assume the branch is not taken, sometimes we'll be right and the
branch will only use 1 cycle, while sometimes we'll be wrong and the branch
will use 3 cycles - this is still better than making no predictions at all. In
this model, regular instructions only take 1 cycle, as well.</p>
<p><img src="lesson4/./img/not-taken-prediction.png" alt="not-taken-prediction" /></p>
<h2><a class="header" href="#predict-not-taken" id="predict-not-taken">Predict not-taken</a></h2>
<p>The <strong>predict not-taken</strong> predictor is the simplest predictor we can have, all
it does is increment the program counter. No extra hardware is required to
support the predict not-taken predictor - we should always have the ability to
increment the program counter.</p>
<p>A rule of thumb in computer architecture is that
<strong>20 percent of all instructions are branches</strong>. For branches, about
<strong>60 percent of all branches are taken</strong>. Thus, the predict not-taken predictor
is correct 80 percent of the time (because of non-branch instructions), plus
another 8 percent of the time (because of branch instructions). Overall, the
predict not-taken predictor is incorrect 12 percent of the time.</p>
<p><img src="lesson4/./img/predict-not-taken.png" alt="predict-not-taken" /></p>
<h2><a class="header" href="#why-do-we-need-better-prediction" id="why-do-we-need-better-prediction">Why do we need better prediction?</a></h2>
<p>The image below provides examples for different types of pipelines, showcasing
the differences in performance using a predict not-taken predictor and a
branch predictor that is correct 99 percent of the time. As you can see, as the
processor pipeline gets longer and more complex, conducting more instructions
per cycle, branch prediction becomes really important for performance. The
ability to accurately predict branches achieves a speedup of 4 for the case
of the 14 stage pipeline with 4 instructions per cycle.</p>
<p><img src="lesson4/./img/better-prediction.png" alt="better-prediction" /></p>
<h2><a class="header" href="#predictor-impact-quiz" id="predictor-impact-quiz">Predictor impact quiz</a></h2>
<p>Below is an example quiz that demonstrates how we can determine the impact
of a predictor on the CPI of a processor pipeline. Given some metrics about the
Pentium 4 processor, we are given a final CPI but not the theoretically ideal
CPI of the pipeline. Using the metrics given, we have to derive the ideal CPI
and then calculate the new CPI given a different mis-prediction percentage for
our predictor.</p>
<p><img src="lesson4/./img/predictor-impact-quiz.png" alt="predictor-impact-quiz" /></p>
<h2><a class="header" href="#why-we-need-better-prediction-part-2" id="why-we-need-better-prediction-part-2">Why we need better prediction, part 2</a></h2>
<p>The below image displays how wasteful flushing the processor pipeline is when
we fail to accurately predict branches. Depending upon how many stages are
within a pipeline, while also accounting for the fact that we could execute
multiple instructions per cycle, we see that would fetch and flush anywhere from
2, to 10, to 40 instructions - pretty wasteful.</p>
<p><img src="lesson4/./img/better-prediction-2.png" alt="better-prediction-2" /></p>
<h2><a class="header" href="#how-do-we-get-better-prediction" id="how-do-we-get-better-prediction">How do we get better prediction?</a></h2>
<p>In a <a href="lesson4/prediction.html#branch-prediction-requirements">previous section</a>, we discussed the
requirements that needed to be met in order to conduct branch prediction. In the
image below, the lecture discusses that we don't have the ability to determine
if the instruction is a branch, if the instruction is taken, or what the offset
of the branch is because we still have yet to fetch the instruction.</p>
<p>What we do know, however, is that, based upon the program counter, we have a
historical precedence for what happened at this program counter in the past. We
know how the branch at this program counter was behaving in the past - we can
use this information because it's pretty common that branches tend to behave the
same way each time they are encountered.</p>
<p>We don't know what the current branch instruction to be fetched is going to do,
but we can make a prediction based upon its history / the last time it was
executed.</p>
<p><img src="lesson4/./img/better-prediction-3.png" alt="better-prediction-3" /></p>
<h1><a class="header" href="#branch-target-buffer-btb" id="branch-target-buffer-btb">Branch Target Buffer (BTB)</a></h1>
<p>The simplest predictor that uses history is the <strong>branch target buffer</strong>. So
how does it work? The program counter (PC) is used to index into the BTB.
Stored at this index is the predicted next program counter. The next program
counter is fetched into the pipeline. When the branch is calculated, the real
next program counter is determined and compared against the predicted next
program counter that was fetched. If they're the same, the BTB predicted the
next program counter correctly based upon the historical data saved for the
program counter used as an index. If they're not the same, the BTB predicted
incorrectly and the next program counter is saved to the BTB using the index
program counter.</p>
<p><img src="lesson4/./img/branch-target-buffer.png" alt="branch-target-buffer" /></p>
<p>How big does the BTB need to be to implement the predictor? Well it serves the
purpose of predicting the next instruction based upon the current instruction
in the program counter. So in order to feasibly predict every next instruction,
we need to have an index for every instruction in the program - meaning the
BTB needs to be the same size as the program itself! That's not realistic, let's
see if we can't find another answer.</p>
<p><img src="lesson4/./img/branch-target-buffer-size.png" alt="branch-target-buffer-size" /></p>
<h2><a class="header" href="#realistic-btb" id="realistic-btb">Realistic BTB</a></h2>
<p>First, we don't need an entry in the BTB for every single instruction in the
program - we can have the BTB contain only the instructions that are most
likely to be executed next. The BTB will operate somewhat like a cache, using
values that are most recently used. An example provided in the lecture is a
loop that contains, say, 100 instructions. As the loop executes, the BTB fills
with the loop's instructions and the BTB doesn't change for the duration of the
loop.</p>
<p>So how do we avoid program counter index collisions in the BTB? We use a
mapping function that's simple enough to execute within one cycle. In the
example provided, if the BTB contains 1024 entries, we use the last 10 bits of
our 64 bit program counters as indices. Why are we using the LSB of our program
counters? Because as a program increments, these values are what changes when
each instruction is executed.</p>
<p><img src="lesson4/./img/realistic-btb.png" alt="realistic-btb" /></p>
<h2><a class="header" href="#btb-quiz" id="btb-quiz">BTB Quiz</a></h2>
<p>In the quiz below, we would think that we would just use the last 10 bits of the
program counter, however, because our architecture enforces 4 bytes instructions
that are word-aligned, each entry in the BTB needs to be even and also divisible
by 4. So, in a BTB of 1024 entries, we would only be using 256 of the values -
that's a lot of wasted entries.</p>
<p>Our solution is to ignore the last 2 bits of the program counter and use the 10
bits to the left of these ignored bits - these bits are the ones that are most
likely to change. Using this indexing function, a program counter of
<code>0x0000ab0c</code> will be contained at BTB index <code>0x2c3</code>.</p>
<p><img src="lesson4/./img/btb-quiz.png" alt="btb-quiz" /></p>
<h1><a class="header" href="#direction-predictor" id="direction-predictor">Direction Predictor</a></h1>
<p>We further simplify our branch predictor by adding another abstraction, the
<strong>branch history table</strong>. This table contains a <code>1</code> or <code>0</code> for each different
program counter that indexes into it - we calculate indices just like we did
with the BTB (using the LSBits of a program counter that are most likely to be
unique).</p>
<p>The <code>0</code> represents that the branch is not taken. The program counter is
incremented and we begin fetching instructions from that location. No changes
are made to the BTB because no branch was taken.</p>
<p>The <code>1</code> represents that the branch is taken. The BTB is referenced to determine
the next program counter based upon the current one being inspected, and we
begin to fetch instructions from that new location.</p>
<p>If the prediction made is incorrect, the branch history table is updated with
the correct value at the index - <code>0</code> or <code>1</code>. If a not taken prediction was made
and this prediction ended up being incorrect, we update the BTB with the next
program counter that is the destination of the branch.</p>
<p>Because the branch history table is only 1 bit for each index, we can have way
more entries for unique program counters. This way, we can use the BTB only for
branches and not just regular instructions.</p>
<p><img src="lesson4/./img/direction-predictor.png" alt="direction-predictor" /></p>
<h2><a class="header" href="#btb--bht-quiz" id="btb--bht-quiz">BTB &amp; BHT Quiz</a></h2>
<p>The below quiz excerpt demonstrates that the BHT needs to be accessed for each
instruction.</p>
<p><img src="lesson4/./img/btb-bht-quiz.png" alt="btb-bht-quiz" /></p>
<h2><a class="header" href="#btb--bht-quiz-2" id="btb--bht-quiz-2">BTB &amp; BHT Quiz 2</a></h2>
<p>The below quiz excerpt demonstrates how the BHT is indexed using the program
counter.</p>
<p><img src="lesson4/./img/btb-bht-quiz-2.png" alt="btb-bht-quiz-2" /></p>
<h2><a class="header" href="#btb--bht-quiz-3" id="btb--bht-quiz-3">BTB &amp; BHT Quiz 3</a></h2>
<p>The below quiz excerpt demonstrates the conditions required to access the BTB.</p>
<p><img src="lesson4/./img/btb-bht-quiz-3.png" alt="btb-bht-quiz-3" /></p>
<h2><a class="header" href="#btb--bht-quiz-4" id="btb--bht-quiz-4">BTB &amp; BHT Quiz 4</a></h2>
<p>The below quiz excerpt demonstrates how the BTB is indexed using the program
counter.</p>
<p><img src="lesson4/./img/btb-bht-quiz-4.png" alt="btb-bht-quiz-4" /></p>
<h2><a class="header" href="#btb--bht-quiz-5" id="btb--bht-quiz-5">BTB &amp; BHT Quiz 5</a></h2>
<p>The below quiz excerpt demonstrates how we can calculate the number of
mis-predictions for each instruction using an imperfect BHT.</p>
<p><img src="lesson4/./img/btb-bht-quiz-5.png" alt="btb-bht-quiz-5" /></p>
<h2><a class="header" href="#problems-with-1-bit-prediction" id="problems-with-1-bit-prediction">Problems with 1 bit prediction</a></h2>
<p>So what's wrong with the 1 bit predictor? Why does its performance not satisfy
the needs of computer architecture designers, creating the need to research
pattern predictors?</p>
<p>The 1 bit predictor predicts these things well:</p>
<ul>
<li><strong>Branches that are always taken</strong></li>
<li><strong>Branches that are always not taken</strong></li>
<li><strong>Branches that are taken A LOT more often than not taken</strong></li>
<li><strong>Branches that are not taken A LOT more often than taken</strong></li>
</ul>
<p>The example below demonstrates how, when a 1 bit predictor encounters an
<strong>anomaly</strong> in predictions for a particular branch, this can incur two
mis-predictions.</p>
<p>The 1 bit predictor does not perform well when:</p>
<ul>
<li><strong>Branches are taken more often than not, but not by much</strong></li>
<li><strong>Branches are not taken more often than not, but not by much</strong></li>
<li><strong>Encountering short loops</strong></li>
</ul>
<p>The 1 bit predictor will be bad when:</p>
<ul>
<li><strong>Branches taken and not taken are about the same in number</strong></li>
</ul>
<p><img src="lesson4/./img/1-bit-predictor-problems.png" alt="1-bit-predictor-problems" /></p>
<h1><a class="header" href="#2-bit-predictor" id="2-bit-predictor">2 Bit Predictor</a></h1>
<p>A 2 bit predictor or 2 bit counter (2BP or 2BC) just adds another bit to the BHT
to implement a state machine. The first bit in the 2BP is the prediction bit,
and the second bit is called the <strong>hysteresis</strong> or conviction bit. These are the
states of the 2BP:</p>
<ul>
<li><code>00</code> - strong not-taken state</li>
<li><code>01</code> - weak not-taken state</li>
<li><code>10</code> - weak taken state</li>
<li><code>11</code> - strong taken state</li>
</ul>
<p>You can probably see where this is going. We start out in the strong not-taken
state for a program counter. When we mis-predict that a branch is not-taken, we
increment the hysteresis bit and transition to the weak not-taken state. We
still don't take the branch until we are proven incorrectly again. A
mis-prediction at this point will transition us to the weak taken state.</p>
<p>So unlike the 1 bit predictor, our affinity to take or not take a branch doesn't
flip as easily. This solves the problem we had earlier where, if we encountered
an anomaly for a branch that is usually taken or not taken, we won't incur a
penalty of two mis-predictions.</p>
<p><img src="lesson4/./img/2-bit-predictor.png" alt="2-bit-predictor" /></p>
<h2><a class="header" href="#2-bit-predictor-initialization" id="2-bit-predictor-initialization">2 Bit Predictor initialization</a></h2>
<p>The below excerpt from the lectures demonstrates that it's a good idea to
initialize the 2 bit predictor in a weak state, doesn't matter if it's taken or
not taken. This is because, at program start, we don't know the history of the
branches, yet, so starting in the weak state only incurs 1 mis-prediction
penalty while starting in a strong state incurs 2.</p>
<p>On the flip-side, if a branch flip-flops and we start in the weak state, we will
incur a mis-prediction penalty every single time we encounter the branch. This
isn't common behavior (fortunately).</p>
<p>The evidence provided above would give you the idea that we should always init
the 2 bit predictor in the weak state, but for the sake of practicality, it
really doesn't matter and most 2 bit predictors are initialized at <code>00</code>.</p>
<p><img src="lesson4/./img/2-bit-predictor-init.png" alt="2-bit-predictor-init" /></p>
<h2><a class="header" href="#2-bit-predictor-quiz" id="2-bit-predictor-quiz">2 Bit Predictor Quiz</a></h2>
<p>The below quiz excerpt from the lectures demonstrates how there is a worst-case
scenario for the 2 bit predictor, showcasing a series of branches that will
cause the predictor to mis-predict every branch.</p>
<p>The moral of the story is, <strong>every</strong> predictor will have a worst-case scenario.
A good predictor will make it difficult to find or generate this worst-case
sequence of branches.</p>
<p><img src="lesson4/./img/2-bit-predictor-quiz.png" alt="2-bit-predictor-quiz" /></p>
<h2><a class="header" href="#1-bits-2-bits-what-else" id="1-bits-2-bits-what-else">1 bits, 2 bits... what else?</a></h2>
<p>So we've seen that the 2 bit predictor is better than the 1 bit predictor at
handling anomalous outcomes. Does this get better if we add 3 bits to the
predictor? Well, maybe. Another bit will just increase the number of states,
making it more difficult to transition to another prediction - this is useful
if the anomalous behavior comes in streaks. This behavior is not common, however
, in programs. Adding another bit imposes more cost, as it requires more space
to store the information per program counter.</p>
<p>A 2 bit predictor meets the needs we identified earlier of handling anomalous
behavior, but what about that one case where the branch outcome kept
flip-flopping? Since increasing the number of bits to solve this problem doesn't
really do anything, what's the next solution?</p>
<p><img src="lesson4/./img/2-bit-predictor-summary.png" alt="2-bit-predictor-summary" /></p>
<h1><a class="header" href="#history-based-predictors" id="history-based-predictors">History Based Predictors</a></h1>
<p>History Based Predictors are designed to recognize and learn patterns based
upon the history of taken or not taken outcomes of a branch. Below is an example
that provides a high-level representation of the concept. The flip-floppy
branch outcomes are definitely predictable, just not with simple N-bit counters.</p>
<p><img src="lesson4/./img/history-based-predictors.png" alt="history-based-predictors" /></p>
<h2><a class="header" href="#1-bit-history-w-2bc" id="1-bit-history-w-2bc">1 Bit History w/ 2BC</a></h2>
<p>Below is an excerpt from the lectures that provides a high-level representation
of a BHT with 1 bit for history, and two 2BC's to manage the state for taken
and not taken branch outcomes.</p>
<p>What the 1 bit history 2BC BHT provides us is a predictor that is able to learn
the pattern of a series of branch outcomes. The 1 bit history bit is used to
index into the two 2BC's and, based upon the branch outcome, updates the state
of each 2BC.</p>
<p><img src="lesson4/./img/1-bit-history-2bc.png" alt="1-bit-history-2bc" /></p>
<h2><a class="header" href="#1-bit-history-quiz" id="1-bit-history-quiz">1 Bit History Quiz</a></h2>
<p>Below is an a quiz excerpt from the lectures demonstrating the calculation of
the number of mis-predictions for a pattern of branch outcomes following the
this pattern: <code>(NNT)*</code>. As we can see from the outcome of the calculations in
the quiz, the 1 Bit History with two 2BC's is not a good solution for this
pattern.</p>
<p><img src="lesson4/./img/1-bit-history-2bc-quiz.png" alt="1-bit-history-2bc-quiz" /></p>
<h2><a class="header" href="#2-bit-history-predictor" id="2-bit-history-predictor">2 Bit History Predictor</a></h2>
<p>The 2 Bit History Predictor works very similar to the 1 Bit History Predictor,
now we've just doubled the number of 2BCs. The 2 Bit History bits are used to
index into the four 2BCs. The total cost of the predictor is 10 bits per branch.
Below is a high-level representation of how a 2 Bit History Predictor can
perfectly predict the <code>(NNT)*</code> sequence.</p>
<p><img src="lesson4/./img/2-bit-history-predictor.png" alt="2-bit-history-predictor" /></p>
<p>Thinking more about our history predictors, some things become evident. For an
N-bit History Predictor:</p>
<ul>
<li>We can perfectly predict all patterns of length less than or equal to <code>N+1</code>.</li>
<li>The cost for an N-bit History Predictor is <code>N + 2 * 2^N</code> <strong>per entry</strong></li>
<li>This is wasteful, most of the 2BCs will go unused.</li>
</ul>
<p><img src="lesson4/./img/2-bit-history-predictor-waste.png" alt="2-bit-history-predictor-waste" /></p>
<h2><a class="header" href="#n-bit-history-predictor-quiz" id="n-bit-history-predictor-quiz">N-bit History Predictor Quiz</a></h2>
<p>The below quiz is an excerpt from the lecture, outlining the cost and waste of
different size N-bit History Predictors.</p>
<p><img src="lesson4/./img/n-bit-history-predictor-quiz.png" alt="n-bit-history-predictor-quiz" /></p>
<h2><a class="header" href="#history-predictor-quiz" id="history-predictor-quiz">History Predictor Quiz</a></h2>
<p>The below quiz is an excerpt from the lecture, demonstrating how to calculate
the number of entries in a history predictor necessary to support a branch
outcome pattern generated by these two <code>for</code> loops. As you can see, a lot of
2BCs are wasted just to support an 8 bit history.</p>
<p><img src="lesson4/./img/history-predictor-quiz.png" alt="history-predictor-quiz" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        
        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>
        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
